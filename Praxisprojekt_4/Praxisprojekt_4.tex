% !TeX program = lualatex
% Für echtes Arial/Calibri: lualatex/xelatex + biber verwenden.
% Fallback mit pdflatex funktioniert (sans: TeX Gyre Heros).

\documentclass[11pt,a4paper]{article}

% ---------- Pakete: Sprache, Schrift, Mikrotypografie ----------
\usepackage[a4paper,margin=2cm]{geometry} % 2,00 cm Ränder
\usepackage[ngerman,english]{babel}       % Silbentrennung DE/EN
\usepackage{microtype}                    % Mikrotypografie (Silbentrennung/Zeilenumbrüche)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{iftex}
\ifPDFTeX
% Fallback: pdfLaTeX (Arial-ähnlich)
\usepackage[scale=0.95]{tgheros}   % TeX Gyre Heros ~ Helvetica/Arial
\renewcommand{\familydefault}{\sfdefault}
\else
% Echte Systemschriften mit Lua/XeLaTeX:
\usepackage{fontspec}
\usepackage{xcolor}
\setmainfont{Arial}[
Ligatures=TeX,
Scale=1.0
]
% Optionaler alternativer Sans-Font:
% \setmainfont{Calibri}[Ligatures=TeX,Scale=1.0]
\fi

% ---------- Zeilen & Absätze ----------
\usepackage{setspace}
\onehalfspacing                           % 1,5-zeilig
\setlength{\parindent}{0pt}               % kein Einzug
\setlength{\parskip}{6pt}                 % 6 pt Abstand nach Absatz

% ---------- Mathe, Grafik, Tabellen ----------
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{subcaption}
\usepackage{siunitx}
\sisetup{detect-all}                      % Sans-Serif in siunitx
\usepackage{csquotes}                     % saubere Anführungszeichen / Blockzitate
\usepackage{mwe}                          % Beispielbilder (example-image*)

% ---------- Code ----------
\usepackage{listings}

% dezente Farben
\definecolor{lwbg}{rgb}{0.98,0.98,0.98}     % leichter Hintergrund
\definecolor{lwkeyword}{rgb}{0.0,0.45,0.30} % keywords (dezent grün)
\definecolor{lwcomment}{rgb}{0.45,0.45,0.45}
\definecolor{lwstring}{rgb}{0.5,0.15,0.15}

\lstdefinestyle{python-academic}{
    language=Python,
    backgroundcolor=\color{lwbg},
    basicstyle=\ttfamily\small,        % gut lesbare Größe im Fließtext
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=8pt,
    frame=single,                      % klarer, aber dezenter Rahmen
    rulecolor=\color{black!20},
    tabsize=4,
    breaklines=true,
    breakatwhitespace=false,
    showstringspaces=false,
    captionpos=b,
    aboveskip=6pt,
    belowskip=6pt,
    xleftmargin=4pt,
    keywordstyle=\bfseries\color{lwkeyword},
    commentstyle=\itshape\color{lwcomment},
    stringstyle=\color{lwstring},
    upquote=true,                      % gerade quotes, falls nötig
    escapeinside={(*@}{@*)},           % für LaTeX-Math etc. im Listing
}

\lstset{style=python-academic}

% Nutzung: entweder inline
% \begin{lstlisting}[caption={Minimalbeispiel},label={lst:mini}]
    % def foo(x):
    %     return x**2
    % \end{lstlisting}
%
% oder aus Datei:
% \lstinputlisting[language=Python, caption={Script X}, label={lst:scriptx}]{path/to/script.py}

% ---------- Bild-/Tabellenunterschriften (10 pt) ----------
\usepackage{caption}
\captionsetup{font=footnotesize,labelfont=bf,labelsep=colon}
\addto\captionsngerman{\renewcommand{\figurename}{Abb.}}
\addto\captionsngerman{\renewcommand{\tablename}{Tab.}}
\newcommand{\source}[1]{\caption*{\footnotesize #1}} % Quellenangabe 10 pt

% ---------- Überschriften: Größen/Abstände/Nummerntiefe ----------
\usepackage{titlesec}
% H1: 16 pt, 12/12
\titleformat{\section}{\bfseries\fontsize{16pt}{18pt}\selectfont}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{12pt}{12pt}
% H2: 14 pt, 12/6
\titleformat{\subsection}{\bfseries\fontsize{14pt}{16pt}\selectfont}{\thesubsection}{0.6em}{}
\titlespacing*{\subsection}{0pt}{12pt}{6pt}
% H3: 11 pt, 12/6
\titleformat{\subsubsection}{\bfseries\fontsize{11pt}{13pt}\selectfont}{\thesubsubsection}{0.6em}{}
\titlespacing*{\subsubsection}{0pt}{12pt}{6pt}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

% Jede Section (Ebene 1) startet auf neuer Seite
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Inhaltsverzeichnis: nur Ebene 1 fett ----------
\usepackage{tocloft}
\renewcommand{\cftsecfont}{\bfseries}
\renewcommand{\cftsecpagefont}{}

% ---------- Fußnoten explizit 10 pt ----------
\makeatletter
\renewcommand\footnotesize{\@setfontsize\footnotesize{10pt}{12pt}}
\makeatother

% ---------- Seitenzahlen: zentriert im Fuß ----------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}

% ---------- Hyperlinks schwarz (keine Farben/Rahmen) ----------
\usepackage[hidelinks]{hyperref}

% ---------- APA7 Literatur (Biber) + deutscher Mapping ----------
\usepackage[
style=apa,
backend=biber,
sorting=nyt,
uniquename=init,
maxcitenames=2, % "et al." ab 3
maxbibnames=99,
doi=true,
url=true,
dateabbrev=false, % Vollständige Datumsangaben
eprint=false, % Unterdrücke eprint-Felder wenn DOI vorhanden
isbn=false, % ISBN normalerweise nicht in APA7
giveninits=true % Nur Initialen für Vornamen
]{biblatex}
\DeclareLanguageMapping{ngerman}{ngerman-apa}

% Zusätzliche APA7-Konfigurationen
\ExecuteBibliographyOptions{maxbibnames=999} % Alle Autoren im Literaturverzeichnis
\ExecuteBibliographyOptions{giveninits=true} % Nur Initialen
\ExecuteBibliographyOptions{uniquename=init} % Eindeutigkeit durch Initialen

% DOI-Formatierung anpassen
\DeclareFieldFormat{doi}{%
  \mkbibacro{DOI}\addcolon\space
  \ifhyperref
    {\href{https://doi.org/#1}{\nolinkurl{#1}}}
    {\nolinkurl{#1}}}

% Hängender Einzug 1.27 cm, 1,5-zeilig wie Text
\setlength{\bibhang}{1.27cm}
\defbibenvironment{bibliography}
{\list
    {\printtext[labelnumberwidth]{\printfield[labelnumberwidth]{labelnumber}}}
    {\setlength{\leftmargin}{\bibhang}
        \setlength{\itemindent}{-\bibhang}
        \setlength{\itemsep}{\baselineskip} % 1.5-Zeilenabstand wie Text
        \setlength{\parsep}{0pt}}
    \renewcommand*{\makelabel}[1]{##1\hss}}
{\endlist}
{\item}

% Beispiel-Bibliothek im Dokument (kannst du ersetzen)
\begin{filecontents*}{\jobname.bib}
    @book{Goodfellow2016,
        author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
        year      = {2016},
        title     = {Deep Learning},
        publisher = {MIT Press},
        address   = {Cambridge, MA},
        isbn      = {978-0262035613}
    }
    @book{Bishop2006,
        author    = {Bishop, Christopher M.},
        year      = {2006},
        title     = {Pattern Recognition and Machine Learning},
        publisher = {Springer},
        address   = {New York, NY},
        doi       = {10.1007/978-0-387-45528-0}
    }
    @book{Hastie2009,
        author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
        year      = {2009},
        title     = {The Elements of Statistical Learning},
        subtitle  = {Data Mining, Inference, and Prediction},
        edition   = {2},
        publisher = {Springer},
        address   = {New York, NY},
        doi       = {10.1007/978-0-387-84858-7}
    }
    @article{Kingma2015,
        author  = {Kingma, Diederik P. and Ba, Jimmy},
        year    = {2015},
        title   = {Adam: A Method for Stochastic Optimization},
        journaltitle = {Proceedings of the 3rd International Conference on Learning Representations},
        venue   = {San Diego, CA},
        eprint  = {1412.6980},
        eprinttype = {arxiv},
        url     = {https://arxiv.org/abs/1412.6980}
    }
    @incollection{Platt1999,
        author    = {Platt, John},
        year      = {1999},
        title     = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
        booktitle = {Advances in Large Margin Classifiers},
        editor    = {Smola, Alexander J. and Bartlett, Peter and Schölkopf, Bernhard and Schuurmans, Dale},
        publisher = {MIT Press},
        address   = {Cambridge, MA},
        pages     = {61--74}
    }
    @online{Mueller2023,
        author = {Müller, Andreas and Schmidt, Maria},
        year   = {2023},
        title  = {Aktuelle Entwicklungen im Machine Learning},
        url    = {https://example.com/ml-trends},
        urldate = {2024-01-15}
    }
\end{filecontents*}
\addbibresource{Praxisprojekt_4.bib}

% ---------- Automatische Verzeichnisse nur bei ≥3 Einträgen ----------
\usepackage{totcount}
\regtotcounter{figure}
\regtotcounter{table}
\newcommand{\addtoTOC}[1]{\addcontentsline{toc}{section}{#1}}
\newcommand{\printlistsconditional}{%
    % Wirksam nach erneutem LaTeX-Lauf (Zähler aus .aux):
    \ifnum\totvalue{figure}>2
    \renewcommand{\listfigurename}{Abbildungsverzeichnis}
    \listoffigures
    \addtoTOC{Abbildungsverzeichnis}
    \clearpage
    \fi
    \ifnum\totvalue{table}>2
    \renewcommand{\listtablename}{Tabellenverzeichnis}
    \listoftables
    \addtoTOC{Tabellenverzeichnis}
    \clearpage
    \fi
}

% ---------- Blockzitat ≥ 40 Wörter (APA) ----------
\newenvironment{blockzitat}{%
    \begin{quote}\setlength{\leftskip}{1.27cm}\itshape\upshape\mdseries\selectfont
    }{\end{quote}}

% ---------- Meta-Felder für Titelseite ----------
\newcommand{\university}{Internationale Hochschule Duales Studium}
\newcommand{\studyprogram}{B.Sc. Informatik}
\newcommand{\thesistype}{Projektarbeit}
\newcommand{\papertitle}{Inwieweit sind Machine-Learning-Modelle für Netzwerk-Anomalieerkennung zwischen verschiedenen Datensätzen übertragbar?}
\newcommand{\authorname}{Jonas Weirauch}
\newcommand{\matno}{10237021}
\newcommand{\address}{Im Wiesengrund 19, 55286 Sulzheim}
\newcommand{\advisor}{Dominic Lindner}
\newcommand{\submissiondate}{30.09.2025}

% ============================================================
%                         DOKUMENT
% ============================================================
\begin{document}
    \selectlanguage{ngerman}

    % ---------- Titelblatt (zählt als I, ohne Zahl) ----------
    \pagenumbering{Roman}
    \setcounter{page}{1}
    \begin{titlepage}
        \thispagestyle{empty}
        \vspace*{-1cm}

        % IU Logo
        \begin{center}
            \includegraphics[width=5cm]{/home/jonas/Documents/Studium/Allgemein/IU_Logo.png}
        \end{center}

        \vspace{3cm}

        % Projektarbeit
        \begin{center}
            {\fontsize{11pt}{13pt}\selectfont \thesistype}
        \end{center}

        \vspace{2cm}

        % University and Program
        \begin{center}
            {\fontsize{11pt}{13pt}\selectfont \university}

            \vspace{0.5cm}

            {\fontsize{11pt}{13pt}\selectfont Studiengang: \studyprogram}
        \end{center}

        \vspace{2cm}

        % Title
        \begin{center}
            {\bfseries\fontsize{12pt}{14pt}\selectfont \papertitle}
        \end{center}

        \vspace{2cm}

        % Author details
        \begin{center}
            {\fontsize{11pt}{13pt}\selectfont \authorname}

            {\fontsize{11pt}{13pt}\selectfont Matrikelnummer: \matno}

            {\fontsize{11pt}{13pt}\selectfont \address}
        \end{center}

        \vspace{2cm}

        % Supervisor and date
        \begin{center}
            {\fontsize{11pt}{13pt}\selectfont Betreuende Person: \advisor}

            {\fontsize{11pt}{13pt}\selectfont Abgabedatum: \submissiondate}
        \end{center}

        \vfill
    \end{titlepage}

    % ---------- Erklärung / Sperrvermerk (optional je nach Arbeit) ----------
%    \section*{Erklärung / Sperrvermerk}
%    \addtoTOC{Erklärung / Sperrvermerk}
%    Hier ggf. die Eigenständigkeits- und Sperrvermerkserklärung gemäß Vorgaben der Hochschule.
%    \clearpage

    % ---------- Danksagung (optional) ----------
%    \section*{Danksagung}
%    \addtoTOC{Danksagung}
%    Optionaler Text für Danksagungen.
%    \clearpage

    % ---------- Abstracts (Deutsch & Englisch, je ca. 200 Wörter) ----------
%    \section*{Abstract (Deutsch)}
%    \addtoTOC{Abstract (Deutsch)}
%    Kurzfassung der Arbeit (ca. 200 Wörter): Problemstellung, Methode, Ergebnisse, Implikationen.
%    \clearpage

%    \begin{otherlanguage*}{english}
%        \section*{Abstract (English)}
%        \addtoTOC{Abstract (English)}
%        Abstract (approx. 200 words): problem, method, results, implications.
%    \end{otherlanguage*}
%    \clearpage

    % ---------- Inhaltsverzeichnis ----------
    \renewcommand{\contentsname}{Inhaltsverzeichnis}
    \tableofcontents
    \clearpage

    % ---------- Abbildungs-/Tabellenverzeichnis (nur bei ≥3) ----------
    \printlistsconditional

    % ---------- Abkürzungsverzeichnis ----------
    \section*{Abkürzungsverzeichnis}
    \addtoTOC{Abkürzungsverzeichnis}
    \begin{tabular}{@{}ll}
        \textbf{AI}           & Artificial Intelligence \\
        \textbf{AUC}          & Area Under the Curve \\
        \textbf{CIC-IDS-2017} & Canadian Institute for Cybersecurity Intrusion Detection System 2017 \\
        \textbf{DoS}          & Denial-of-Service \\
        \textbf{EFB}          & Exclusive Feature Bundling \\
        \textbf{FPR}          & False Positive Rate \\
        \textbf{GOSS}         & Gradient-based One-Side Sampling \\
        \textbf{HIDS}         & Host-based Intrusion Detection Systems \\
        \textbf{IDS}          & Intrusion Detection Systems \\
        \textbf{k-NN}         & k-Nearest Neighbors \\
        \textbf{ML}           & Machine Learning \\
        \textbf{MLP}          & Multi-Layer Perceptron \\
        \textbf{NIDS}         & Network-based Intrusion Detection Systems \\
        \textbf{NSL-KDD}      & Network Security Laboratory - Knowledge Discovery and Data Mining \\
        \textbf{ROC}          & Receiver Operating Characteristic \\
        \textbf{SVM}          & Support Vector Machines \\
        \textbf{TPR}          & True Positive Rate \\
    \end{tabular}
    \clearpage

    % ---------- Hauptteil: arabische Seitenzahlen ab "Einleitung" ----------
    \pagenumbering{arabic}
    \setcounter{page}{1}

    \section{Einleitung}
    \subsection{Motivation und Problemstellung}

    Mit über 10,5 Billionen US-Dollar geschätzten jährlichen Schäden bis 2025 stellen Cyberangriffe eine der größten globalen Bedrohungen dar \parencite{GlobalRisksReport2024}. Diese besorgniserregenden Statistiken unterstreichen die akute Notwendigkeit wirksamer Sicherheitsvorkehrungen zum Schutz kritischer Infrastrukturen \parencite{Taman2024}.

    Traditionelle signaturbasierte Intrusion Detection Systeme (IDS) erreichen zunehmend ihre Grenzen bei der Erkennung neuartiger Zero-Day-Exploits und unbekannter Angriffsmuster \parencite{Ring2019,Belavagi2016}. Machine Learning (ML) bietet das Potenzial, diese Limitationen zu überwinden, jedoch ist die tatsächliche Wirksamkeit verschiedener ML-Modelle in heterogenen Netzwerken noch nicht vollständig geklärt. Ein kritisches Problem stellt dabei die Generalisierungsfähigkeit dar: Während Modelle auf spezifischen Trainingsdaten exzellente Leistungen erzielen, zeigen sie oft dramatische Leistungseinbußen beim Transfer auf neue Netzwerkumgebungen \parencite{Ring2019}.

    \textbf{Forschungslücke}: Bisherige Studien konzentrieren sich primär auf Within-Dataset-Evaluationen und vernachlässigen die praktisch relevante Frage der Cross-Dataset-Transferierbarkeit \parencite{Mourouzis2021}. Die systematische Bewertung der Generalisierungsfähigkeit zwischen fundamental verschiedenen Netzwerk-Datensätzen, insbesondere zwischen historischen Benchmarks wie NSL-KDD und modernen Datensätzen wie CIC-IDS-2017, bleibt eine unzureichend erforschte, aber praxiskritische Herausforderung.

    \subsection{Forschungsfrage und Zielsetzung}

    Diese Arbeit untersucht systematisch die Generalisierungsfähigkeit von zwölf ML-Modellen über zwei fundamental unterschiedliche Netzwerk-Datensätze hinweg. Die zentrale Forschungsfrage lautet:

    \textit{„Inwieweit sind Machine-Learning-Modelle für Netzwerk-Anomalieerkennung zwischen verschiedenen Datensätzen übertragbar?"}

    Die Untersuchung fokussiert sich auf die Cross-Dataset-Transferierbarkeit zwischen NSL-KDD (1998, 41 Features) und CIC-IDS-2017 (2017, 79 Features), die sich fundamental in Datenverteilung, Merkmalsdimensionalität und Angriffsszenarien unterscheiden \parencite{Mourouzis2021}.

    Die konkreten Forschungsziele umfassen erstens die \textbf{systematische Cross-Dataset-Evaluation} durch bidirektionale Transferanalyse mit zwölf ML-Algorithmen von Baseline-Modellen (Random Forest, Decision Tree, k-NN, Logistic Regression, Naive Bayes, Linear SVM) bis zu Advanced-Modellen (XGBoost, LightGBM, Gradient Boosting, Extra Trees, MLP, Voting Classifier). Zweitens erfolgt die \textbf{Entwicklung neuartiger Transfer-Metriken} durch Einführung von Generalization Gap, Transfer Ratio und Relative Performance Drop als quantitative Maße für Cross-Dataset-Robustheit. Drittens wird eine \textbf{Feature-Space-Harmonisierung} zur Überbrückung der Dimensionalitätslücke (41 vs. 79 Dimensionen) implementiert. Viertens zielt die Arbeit auf \textbf{praktische Deployment-Guidance} durch Identifikation der transferrobustesten Algorithmen für heterogene Netzwerkumgebungen ab.



    \subsection{Aufbau der Arbeit}

    Die Arbeit gliedert sich in vier aufeinander aufbauende Hauptteile. Zunächst werden in den \textit{theoretischen Grundlagen} die konzeptionellen Fundamente der Netzwerk-Anomalieerkennung etabliert, einschließlich einer Taxonomie der eingesetzten Machine-Learning-Verfahren \parencite{McHugh2000,Vinayakumar2019}.

    Im \textit{methodischen Teil} wird das dreistufige Evaluationsframework vorgestellt, das Within-Dataset-Validation, Cross-Dataset-Transfer und Feature-Harmonisierung systematisch kombiniert \parencite{Gharib2016}.

    Die \textit{empirische Analyse} präsentiert die Ergebnisse der umfassenden Modellvergleiche zwischen NSL-KDD und CIC-IDS-2017. Neben klassischen Performance-Metriken werden neuartige Transfer-Kennzahlen wie Generalization Gap und Transfer Ratio eingeführt.

    Abschließend werden in der \textit{Diskussion} die praktischen Implikationen für IDS-Deployments erörtert. Der wissenschaftliche Beitrag liegt in der erstmaligen systematischen Cross-Dataset-Evaluation von zwölf ML-Modellen unter realistischen Transferbedingungen sowie der Entwicklung neuartiger Transfer-Metriken für ML-basierte Cybersecurity-Systeme.


    \section{Theoretische Fundierung}

    \subsection{Grundlagen der Netzwerk-Anomalieerkennung und Intrusion Detection Systems}

    Die Erkennung von Anomalien im Netzwerkverkehr stellt einen fundamentalen Baustein moderner Cybersicherheitsarchitekturen dar. Intrusion Detection Systems (IDS) fungieren als Frühwarnsysteme, die darauf ausgelegt sind, ungewöhnliche Muster im Netzwerkverkehr zu identifizieren, welche  auf potenzielle Sicherheitsbedrohungen hindeuten könnten \parencite{Ring2019}. Diese Systeme operieren kontinuierlich im Hintergrund und analysieren den gesamten Datenfluss einer Netzwerkinfrastruktur, um Angriffe wie Denial-of-Service (DoS), unbefugtes Eindringen, Datenexfiltration oder Malware-Aktivitäten zu erkennen \parencite{Vinayakumar2019}.

    \textbf{Architektonische Klassifikation von IDS} erfolgt primär nach zwei Dimensionen: dem Einsatzort und der Detektionsmethodik \parencite{Ring2019}. \textbf{Network-based IDS (NIDS)} überwachen den Netzwerkverkehr an strategischen Punkten und analysieren Pakete in Echtzeit, während \textbf{Host-based IDS (HIDS)} direkt auf einzelnen Systemen implementiert werden und Systemlogs, Dateizugriffe und Prozessaktivitäten überwachen. \textbf{Hybrid-Systeme} kombinieren beide Ansätze zur Maximierung der Abdeckung und Minimierung blinder Flecken \parencite{Gharib2016}. Die Wahl der Architektur beeinflusst fundamental die verfügbaren Feature-Sets und damit die Anwendbarkeit verschiedener ML-Algorithmen.

    \textbf{Deployment-Modi} unterscheiden zwischen passiver Überwachung durch Mirroring von Netzwerktraffic und aktiver Inline-Implementierung mit direkter Paketfilterung. Passive Systeme bieten den Vorteil der Latenz-neutralen Überwachung, während Inline-Systeme proaktive Threat-Mitigation ermöglichen, jedoch Durchsatz-Limitationen unterliegen \parencite{Vinayakumar2019}. Diese architektonischen Entscheidungen determinieren die verfügbaren Datencharakteristika und beeinflussen die Generalisierbarkeit trainierter Modelle zwischen verschiedenen Netzwerkumgebungen.

    Die theoretische Grundlage der Anomalieerkennung basiert auf der systematischen Unterscheidung zwischen normalem und abnormalem Netzwerkverhalten. Dabei lassen sich drei fundamentale Kategorien von Anomalien differenzieren \parencite{Ring2019}. \textbf{Punktuelle Anomalien} bezeichnen einzelne Datenpunkte, die signifikant von der erwarteten Normalverteilung abweichen, wie beispielsweise ungewöhnlich hohe Bandbreitennutzung durch einzelne Verbindungen. \textbf{Kontextuelle Anomalien} sind Datenpunkte, die nur unter Berücksichtigung ihres spezifischen Kontexts als anormal klassifiziert werden können. Ein hoher Datenverkehr während Nachtstunden könnte kontextuell anomal sein, obwohl derselbe Verkehr während der Geschäftszeiten normal erscheint. \textbf{Kollektive Anomalien} beziehen sich auf Gruppen von Datenpunkten, die gemeinsam ein ungewöhnliches Verhalten zeigen, obwohl einzelne Werte innerhalb normaler Parameter liegen könnten, wie etwa koordinierte Botnet-Aktivitäten \parencite{Ring2019}.

    Die praktische Implementierung von IDS erfordert jedoch mehr als nur die technische Fähigkeit zur Mustererkennung. Moderne Netzwerkumgebungen sind durch hohe Dynamik, heterogene Infrastrukturen und kontinuierlich evolvierende Bedrohungslandschaften charakterisiert \parencite{Gharib2016}. Dies führt zu dem Phänomen des \textbf{Concept Drift}, bei dem sich die statistische Verteilung der Netzwerkdaten über die Zeit verändert, was die Anpassungsfähigkeit und Generalisierungsfähigkeit der eingesetzten Detektionssysteme vor erhebliche Herausforderungen stellt \parencite{Ring2019}.


    \subsection{Traditionelle versus Machine Learning-basierte Detektionsansätze}

    Die Evolution der Anomalieerkennungstechnologien lässt sich in zwei fundamentale Paradigmen unterteilen: signaturbasierte und anomaliebasierte Verfahren, wobei letztere zunehmend durch Machine Learning-Ansätze implementiert werden \parencite{Ring2019, Belavagi2016}.

    \textbf{Signaturbasierte Systeme} operieren nach dem Prinzip des Musterabgleichs und vergleichen den aktuellen Netzwerkverkehr mit einer Datenbank bekannter Angriffssignaturen \parencite{Ring2019}. Diese Systeme zeichnen sich durch hohe Präzision bei der Erkennung bereits katalogisierter Bedrohungen aus und generieren typischerweise niedrige False-Positive-Raten. Die fundamentale Limitation signaturbasierter Ansätze liegt jedoch in ihrer Reaktivität: Sie können ausschließlich Angriffe identifizieren, deren Signaturen bereits in der Datenbank hinterlegt sind \parencite{Vinayakumar2019}. Diese Eigenschaft macht sie anfällig für Zero-Day-Exploits, polymorphe Malware und neuartige Angriffstechniken, die noch nicht in den Signaturdatenbanken erfasst sind.

    \textbf{Anomaliebasierte Systeme} verfolgen einen proaktiven Ansatz, indem sie zunächst ein statistisches Modell des normalen Netzwerkverhaltens etablieren und anschließend Abweichungen von diesem Baseline-Verhalten als potenzielle Bedrohungen klassifizieren \parencite{Ring2019}. Der entscheidende Vorteil dieses Paradigmas liegt in der theoretischen Fähigkeit zur Detektion unbekannter Angriffsmuster und Zero-Day-Exploits \parencite{Vinayakumar2019}. Allerdings erfordert die praktische Umsetzung eine präzise Modellierung des Normalverhaltens sowie die Definition geeigneter Schwellenwerte zur Minimierung von False-Positive-Meldungen.

    Machine Learning-basierte Ansätze haben das Potenzial, die Limitationen beider traditioneller Paradigmen zu überwinden. Überwachte Lernverfahren können komplexe, nichtlineare Beziehungen zwischen Netzwerkfeatures und Angriffskategorien erlernen, während unüberwachte Methoden in der Lage sind, neuartige Anomaliemuster ohne vorherige Kennzeichnung zu identifizieren \parencite{Vinayakumar2019}. Die Integration von Deep Learning-Techniken ermöglicht zudem die automatische Feature-Extraction aus hochdimensionalen Netzwerkdaten, wodurch manuell entwickelte Heuristiken obsolet werden \parencite{Goodfellow2016}.

    \subsection{Machine Learning-Taxonomie für Anomalieerkennung}

    Die systematische Evaluation von ML-Verfahren in der Netzwerk-Anomalieerkennung erfordert eine strukturierte Kategorisierung nach Komplexität und methodischen Ansätzen. Diese Arbeit implementiert eine zweigeteilte Evaluationsstrategie mit sechs Baseline-Modellen und sechs Advanced-Modellen, um sowohl etablierte als auch moderne Verfahren zu bewerten \parencite{Vinayakumar2019}.

    \textbf{Baseline-Modelle} repräsentieren etablierte, interpretierbare Algorithmen mit moderater Komplexität und geringen computational Anforderungen. \textbf{Random Forest} implementiert Ensemble-Learning durch Bootstrap Aggregating (Bagging) von Entscheidungsbäumen und reduziert Overfitting durch Diversifikation \parencite{Hastie2009}. Die theoretische Robustheit basiert auf dem Law of Large Numbers: Die Aggregation unkorrelierter Schätzer reduziert die Gesamtvarianz proportional zur Anzahl der Bäume. \textbf{Decision Tree} bietet maximale Interpretierbarkeit durch hierarchische if-then-Regeln, neigt jedoch zu Overfitting bei komplexen Datensätzen ohne Regularisierung \parencite{Hastie2009}.

    \textbf{Logistic Regression} modelliert Klassenwahrscheinlichkeiten durch die Sigmoid-Funktion $P(y=1|x) = \frac{1}{1+e^{-(\beta_0 + \beta_1 x)}}$ und ermöglicht probabilistische Klassifikationsentscheidungen mit linearen Entscheidungsgrenzen \parencite{Bishop2006}. Die computational Effizienz macht das Verfahren ideal für Echtzeit-IDS, limitiert jedoch die Modellierung nichtlinearer Feature-Interaktionen. \textbf{Naive Bayes} basiert auf dem Bayes'schen Theorem unter der Unabhängigkeitsannahme $P(x_1,...,x_n|y) = \prod_{i=1}^{n} P(x_i|y)$ \parencite{Bishop2006}. Trotz der oft verletzten Unabhängigkeitsannahme zeigt der Algorithmus robuste Performance bei hochdimensionalen Netzwerk-Features.

    \textbf{k-Nearest Neighbors (k-NN)} implementiert instanzbasiertes Lernen ohne explizites Modelltraining und klassifiziert basierend auf der Mehrheitsentscheidung der k nächsten Nachbarn im Feature-Space \parencite{Bishop2006}. Die Curse of Dimensionality führt jedoch zu Performance-Degradation in hochdimensionalen Netzwerkdaten, da alle Punkte nahezu äquidistant werden \parencite{Hastie2009}. \textbf{Support Vector Machines (Linear SVM)} maximieren den Margin zwischen Klassen durch Optimierung der Hyperebene $w^T x + b = 0$ \parencite{Platt1999}. Die lineare Kernelfunktion bietet computational Effizienz bei großen Datensätzen, jedoch ohne nichtlineare Separierbarkeit.

    \textbf{Advanced-Modelle} repräsentieren moderne, hochperformante Algorithmen mit erhöhter Modellkomplexität und superior Generalisierungsfähigkeit. \textbf{XGBoost (Extreme Gradient Boosting)} implementiert optimiertes Gradient Boosting mit erweiterten Regularisierungstechniken \parencite{Hastie2009}. Die Zielfunktion $\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)$ kombiniert Verlustfunktion mit Regularisierungsterm $\Omega(f_k)$ zur Overfitting-Kontrolle. Jeder neue Baum $f_t$ minimiert die Residuen der vorherigen Iteration: $\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \epsilon f_t(x_i)$.

    \textbf{LightGBM} erweitert Gradient Boosting durch Gradient-based One-Side Sampling (GOSS) und Exclusive Feature Bundling (EFB) \parencite{Zhou2020}. GOSS retainiert Samples mit großen Gradienten und sampelt zufällig aus kleinen Gradienten, wodurch Trainingseffizienz bei erhaltener Accuracy erreicht wird. EFB bündelt sparse Features zur Dimensionsreduktion ohne Informationsverlust. \textbf{Gradient Boosting} implementiert die klassische Sequential-Ensemble-Strategie durch iterative Addition schwacher Lerner zur Residuen-Minimierung \parencite{Hastie2009}.

    \textbf{Extra Trees (Extremely Randomized Trees)} erweitert Random Forest durch zusätzliche Randomisierung in der Split-Punkt-Auswahl \parencite{Hastie2009}. Anstatt optimal Splits zu suchen, werden Split-Punkte zufällig gewählt, was Trainingszeit reduziert und Overfitting minimiert. \textbf{Multi-Layer Perceptron (MLP)} implementiert universelle Funktionsapproximation durch mehrschichtige neuronale Architekturen mit nichtlinearen Aktivierungsfunktionen \parencite{Goodfellow2016}. Die Backpropagation optimiert Gewichte durch Gradientenabstieg: $w_{ij}^{(l)} \leftarrow w_{ij}^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}$.

    \textbf{Voting Classifier} kombiniert heterogene Basis-Lerner durch Soft-Voting zur Vorhersageaggregation: $\hat{y} = \arg\max_c \sum_{i=1}^{m} w_i \cdot P_i(c|x)$, wobei $P_i(c|x)$ die Klassenwahrscheinlichkeiten des i-ten Modells repräsentieren \parencite{Hastie2009}. Die Diversität zwischen Ensemble-Mitgliedern (Tree-based, Boosting, Neural Network) maximiert die Bias-Variance-Dekomposition und verbessert Generalisierungsrobustheit.

    \subsection{Feature Engineering und Datenvorverarbeitung}

    Die Qualität der Feature-Repräsentation determiniert fundamental die Performance der zwölf evaluierten ML-Algorithmen \parencite{Gharib2016}. \textbf{NSL-KDD Features} umfassen 41 Dimensionen mit kategorialen (Protokoll-Typ, Service, Flag) und numerischen Attributen (Dauer, Bytes, Paketanzahl), während \textbf{CIC-IDS-2017} 79 Flow-basierte Features wie Inter-Arrival-Time-Statistiken und Paket-Size-Distributionen bereitstellt \parencite{Sharafaldin2018}.

    \textbf{Skalierung und Normalisierung} sind kritisch für distanzbasierte Algorithmen (k-NN, SVM) und neuronale Netze (MLP) \parencite{Bishop2006}. Min-Max-Skalierung transformiert Features in [0,1]: $x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}$, während Z-Score-Normalisierung Standardnormalverteilung erzeugt: $x_{std} = \frac{x - \mu}{\sigma}$. Tree-basierte Modelle (Random Forest, Decision Tree, XGBoost, LightGBM) sind skalierungsinvariant und erfordern keine Vorverarbeitung.

    \textbf{Klassenimbalance} stellt eine zentrale Herausforderung dar, da normale Verbindungen 95-99\% der Samples ausmachen \parencite{Ring2019}. \textbf{Class Weight Balancing} in Ensemble-Modellen (XGBoost, LightGBM) verwendet inverse Klassenfrequenzen: $w_c = \frac{n_{samples}}{n_{classes} \cdot n_{samples\_c}}$. Probabilistische Modelle (Logistic Regression, Naive Bayes) profitieren von Threshold-Tuning zur Optimierung der Precision-Recall-Balance \parencite{Hastie2009}.

    \subsection{Transfer Learning und Cross-Dataset-Generalisierung}

    Die Transferierbarkeit von Machine Learning-Modellen zwischen verschiedenen Datensätzen stellt eine der zentralen Herausforderungen in der praktischen Anwendung von Anomalieerkennungssystemen dar. \textbf{Transfer Learning} definiert die Fähigkeit eines Systems, Wissen aus einer Quelldomäne zu nutzen, um die Performance in einer verwandten Zieldomäne zu verbessern \parencite{Goodfellow2016}. Im Kontext der Netzwerk-Anomalieerkennung manifestiert sich diese Problematik in der Frage, inwieweit Modelle, die auf einem spezifischen Datensatz trainiert wurden, auf andere Netzwerkumgebungen oder zeitlich versetzte Datenverteilungen generalisieren können.

    \textbf{Domain Adaption} beschreibt den systematischen Transfer von Lernmodellen zwischen Quell- und Zieldomänen, die durch unterschiedliche Datenverteilungen charakterisiert sind \parencite{Goodfellow2016}. In der Praxis unterscheiden sich Netzwerk-Datensätze fundamental in ihrer \textbf{Feature-Dimensionalität} (NSL-KDD: 41 Features vs. CIC-IDS-2017: 79 Features), \textbf{temporalen Abdeckung} (historische vs. moderne Angriffsmuster) und \textbf{Netzwerktopologie} (simulierte vs. reale Umgebungen). Diese Divergenzen führen zu \textbf{Distribution Shift}, einem Phänomen, bei dem die Joint-Probability-Distribution P(X,Y) zwischen Training und Test differiert.

    Die \textbf{Generalisierungslücke} quantifiziert die Performance-Degradation beim Transfer zwischen Datensätzen und lässt sich formal definieren als:

    \begin{equation}
        \text{Generalization Gap} = \text{Performance}_{\text{source}} - \text{Performance}_{\text{target}}
    \end{equation}

    \textbf{Concept Drift} beschreibt zeitliche Veränderungen in der zugrundeliegenden Datenverteilung, die besonders relevant für die Cybersicherheit sind, da sich Angriffstechniken kontinuierlich weiterentwickeln \parencite{Ring2019}. \textbf{Covariate Shift} tritt auf, wenn sich die Eingabedatenverteilung $P(X)$ ändert, während die bedingte Verteilung $P(Y|X)$ konstant bleibt. \textbf{Prior Probability Shift} bezeichnet Veränderungen in der Klassenverteilung $P(Y)$, während \textbf{Concept Shift} fundamentale Änderungen in der Beziehung $P(Y|X)$ beschreibt.

    \textbf{Cross-Dataset-Robustheit} erfordert die Entwicklung von Metriken, die über traditionelle Within-Dataset-Evaluationen hinausgehen. Die \textbf{Transfer Ratio} quantifiziert die relative Performance-Retention:

    \begin{equation}
        \text{Transfer Ratio} = \frac{\text{Performance}_{\text{cross-dataset}}}{\text{Performance}_{\text{within-dataset}}}
    \end{equation}

    Werte nahe 1.0 indizieren hohe Transferierbarkeit, während niedrige Werte auf domänenspezifische Überanpassung hindeuten. Die theoretische Erwartung basiert auf der Hypothese, dass robuste Algorithmen invariante Feature-Repräsentationen erlernen, die weniger anfällig für Domain-Specific-Bias sind.

    Die \textbf{Wasserstein-Distanz} bietet eine theoretisch fundierte Metrik zur Quantifizierung der Divergenz zwischen Datenverteilungen und ermöglicht die systematische Analyse der Domain-Gap zwischen NSL-KDD und CIC-IDS-2017. Diese Distanz-basierte Analyse kann prädiktive Insights bezüglich der erwarteten Transfer-Performance verschiedener Algorithmus-Klassen liefern.

    \subsection{Evaluationsmetriken und Cross-Dataset-Transferierbarkeit}

    Die Bewertung der zwölf ML-Modelle erfordert IDS-spezifische Metriken, die Klassenimbalance und praktische Deployment-Anforderungen berücksichtigen \parencite{Belavagi2016}. \textbf{Accuracy} kann bei imbalancierten Datensätzen irreführend sein, da ein "always normal"-Klassifikator bereits 95\% Accuracy erreicht. \textbf{F1-Score} harmonisiert Precision und Recall: $F_1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$ und bietet ausgewogene Performance-Bewertung \parencite{Hastie2009}.

    \textbf{Cross-Dataset-Transferierbarkeit} quantifiziert die Generalisierungsfähigkeit zwischen NSL-KDD und CIC-IDS-2017 durch neuartige Transfer-Metriken. Die \textbf{Transfer Ratio} misst relative Performance-Retention: $TR = \frac{Performance_{cross}}{Performance_{within}}$, wobei Werte nahe 1.0 hohe Transferierbarkeit indizieren \parencite{Mourouzis2021}. Die \textbf{Generalization Gap} quantifiziert absolute Performance-Degradation: $GG = Performance_{source} - Performance_{target}$.

    \textbf{Computational Efficiency} wird durch Trainings- und Inferenzzeiten charakterisiert, kritisch für Echtzeit-IDS-Deployments. Ensemble-Modelle (XGBoost, LightGBM) bieten optimale Balance zwischen Accuracy und Effizienz, während k-NN hohe Inferenzzeiten bei großen Trainingsdatensätzen aufweist \parencite{Vinayakumar2019}. \textbf{5-Fold Cross-Validation} mit zeitlicher Stratifizierung verhindert Data Leakage und respektiert temporale Abhängigkeiten in Netzwerkdaten \parencite{Tavallaee2009}.

    \section{Methodik}

    \subsection{Forschungsdesign und methodische Begründung}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - FORSCHUNGSDESIGN
    %
    % Grundlegendes Evaluationsframework: Dreistufiges Validierungsprotokoll (Within-Dataset → Cross-Dataset → Feature-Harmonized Transfer)
    % - Begründung: Systematische aufbauende Komplexitätssteigerung gewährleistet vollständige Charakterisierung der Generalisierungsfähigkeit
    % - von einfachen intra-dataset Szenarien bis hin zu realistischen cross-domain Transfer-Situationen
    % - Methodische Innovation: Erste systematische Evaluation der bidirektionalen Transfer-Asymmetrie zwischen Historical/Modern Datasets
    %
    % Bidirektionale Transfer-Evaluation: NSL-KDD ↔ CIC-IDS-2017 mit asymmetrischer Transferanalyse
    % - Begründung: Erkennung von Generalisierungsasymmetrien zwischen historischen (2009) und modernen (2017) Netzwerkumgebungen
    % - ermöglicht differenzierte Bewertung der Temporal-Transfer-Robustheit für praktische IDS-Deployment-Szenarien
    % - Wissenschaftliche Neuheit: Erste quantitative Charakterisierung der 19-Jahre Technology-Gap Impact auf ML-Generalization
    %
    % Reproduzierbarkeitsprotokoll: Deterministische Random Seeds (RANDOM_STATE=42), Environment Validation, Requirements Pinning
    % - Begründung: Vollständige wissenschaftliche Reproduzierbarkeit durch standardisierte Experimentalumgebung
    % - mit dokumentierter Systemkonfiguration und bit-level deterministic execution
    % - Compliance mit Open Science Standards für collaborative research und peer review validation
    %
    % Automatisierte 8-stufige Experimentalpipeline: Von EDA bis Publication-Ready Visualizations
    % - Begründung: Eliminierung manueller Fehlerquellen durch vollständig automatisierte, nachvollziehbare Ausführung
    % - mit Fault-Tolerance und Incremental Recovery für robuste long-running computational experiments
    % - Skalierbarkeit: Memory-adaptive execution von sample-based bis full-dataset processing (2.8M+ samples)

    Die vorliegende Arbeit verfolgt ein \textbf{dreistufiges quantitatives Evaluationsframework}, das systematisch von einfachen Within-Dataset-Validierungen über bidirektionale Cross-Dataset-Transfers bis hin zu Feature-harmonisierten Generalisierungstests fortschreitet. Diese aufbauende Komplexitätssteigerung ermöglicht eine vollständige Charakterisierung der Transferierbarkeit von Machine-Learning-Modellen zwischen historischen (NSL-KDD, 2009) und modernen (CIC-IDS-2017, 2017) Netzwerkumgebungen. Der methodische Ansatz ist ausschließlich quantitativ ausgerichtet, da die Forschungsfrage - \textit{"Inwieweit sind Machine-Learning-Modelle für Netzwerk-Anomalieerkennung zwischen verschiedenen Datensätzen übertragbar?"} - eine messbare, vergleichbare Bewertung von Generalisierungsmetriken erfordert.

    Die \textbf{Wahl eines quantitativen Designs} begründet sich in der Notwendigkeit, objektive Performance-Differenzen zwischen zwölf ML-Algorithmen unter kontrollierten experimentellen Bedingungen zu quantifizieren. Qualitative Ansätze (z. B. Experteninterviews zur Einschätzung von Modellrobustheit) würden subjektive Urteile einführen und die wissenschaftliche Reproduzierbarkeit einschränken. Stattdessen erlaubt die vollständig automatisierte Experimentalpipeline eine bias-freie Evaluation mit deterministischen Ergebnissen (RANDOM\_STATE=42 für alle stochastischen Komponenten).

    Das Forschungsdesign basiert auf drei hierarchischen Evaluationsebenen. Zunächst erfolgt eine \textbf{Within-Dataset-Validation}, bei der die Baseline-Performance der Modelle mithilfe einer 5-fach stratifizierten Kreuzvalidierung auf dem jeweiligen Trainingsdatensatz bestimmt wird. Dadurch entstehen Referenzwerte für die spätere Transferbewertung und es wird sichergestellt, dass beobachtete Performanceeinbußen im Cross-Dataset-Setting tatsächlich auf Domain-Shift zurückzuführen sind und nicht auf inhärente algorithmische Schwächen \parencite{Tavallaee2009}.

    Im nächsten Schritt folgt die \textbf{Cross-Dataset-Transfer-Analyse}, die das Kernexperiment darstellt. Hierbei werden Modelle, die auf NSL-KDD trainiert wurden, auf CIC-IDS-2017 getestet und umgekehrt, sodass eine bidirektionale Evaluation entsteht. Diese Analyse deckt potenzielle Asymmetrien auf und erlaubt die Untersuchung, ob die Transfer-Richtung (historisch → modern vs. modern → historisch) die Generalisierungsfähigkeit beeinflusst. Die Ergebnisse haben unmittelbare praktische Relevanz für den Einsatz von Intrusion Detection Systemen in Bedrohungsumgebungen, die sich stetig weiterentwickeln \parencite{Ring2019}.

    Abschließend wird im Rahmen des \textbf{Feature-Harmonized Transfer} die fundamentale Inkompatibilität der Feature-Räume adressiert (NSL-KDD: 41 Dimensionen; CIC-IDS-2017: 79 Dimensionen). Zu diesem Zweck kommt eine PCA-basierte Alignment-Strategie zum Einsatz, bei der beide Datensätze auf einen gemeinsamen 20-dimensionalen latenten Raum projiziert werden. Da dabei über 95 \% der Varianz erhalten bleiben, ermöglicht dieses Vorgehen eine faire Vergleichbarkeit ohne Verzerrungen durch Feature-Engineering \parencite{Goodfellow2016}.

    Die methodische \textbf{Innovation} liegt in der erstmaligen systematischen Quantifizierung von Cross-Dataset-Generalisierung mittels neuartiger Transfer-Metriken: \textbf{Transfer Ratio} ($\text{TR} = \frac{\text{Performance}_{\text{source}}}{\text{Performance}_{\text{target}}}$) zur Messung relativer Robustheit, \textbf{Generalization Gap} ($\text{GG} = \text{Performance}_{\text{source}} - \text{Performance}_{\text{target}}$) für absolute Degradation und \textbf{Relative Performance Drop} ($\text{RPD} = \frac{\text{GG}}{\text{Performance}_{\text{source}}} \times 100$) für modellübergreifende Vergleichbarkeit \parencite{Mourouzis2021}. Diese Metriken ermöglichen eine theoretisch fundierte Bewertung der praktischen Deployability von ML-Modellen in heterogenen Netzwerkumgebungen.

    Die \textbf{Reproduzierbarkeit} wird durch strikte Einhaltung wissenschaftlicher Standards gesichert: Alle Experimente nutzen die offiziellen Datensatz-Splits (NSL-KDD: 125.973 Train / 22.544 Test), deterministisches Random-Seeding und eine versionierte Software-Umgebung (Python 3.8+, scikit-learn 1.3+, XGBoost 1.7+, LightGBM 3.3+). Die vollständig automatisierte 8-stufige Pipeline eliminiert manuelle Interventionen und gewährleistet Inter-Operator-Reliabilität.

    \subsection{Datengrundlage und Stichprobenauswahl}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - DATENGRUNDLAGE
    %
    % NSL-KDD Datensatz (2009): 125.973 Trainingssamples, 22.544 dedizierte Testsamples
    % - Begründung: De-facto Standard Benchmark mit eliminiertem KDD Cup 99 Duplikate-Bias
    % - etablierte Vergleichbarkeit mit internationaler Forschungslandschaft für Meta-Analysis-Kompatibilität
    % - Repräsentativität: 4 Angriffskategorien (DoS: 36%, Probe: 11%, R2L/U2R: <1%) mit realistischer Class-Imbalance
    % - Feature-Space: 41 Connection-basierte Features mit interpretierbare Network-Flow-Semantik
    % - Zugang: Öffentlich verfügbar, Canadian Institute for Cybersecurity, keine Privacy-Constraints
    %
    % CIC-IDS-2017 Datensatz (2017): 2.8M+ Samples aus realistischen Netzwerkumgebungen
    % - Begründung: Moderne, realitätsnahe Angriffsszenarien mit zeitgemäßen Netzwerkprotokollen
    % - komplementär zu NSL-KDD für 19-Jahre Technologie-Gap Analyse und Temporal-Transfer-Assessment
    % - Attack-Taxonomie: 14 moderne Angriffsfamilien (Heartbleed, SQL Injection, XSS, Botnet, DDoS-Variants)
    % - Feature-Space: 79 bidirektionale Flow-Features mit advanced statistical flow characterization
    % - Realismus: Captured from live network traffic, nicht simuliert, höhere External Validity
    %
    % Memory-Adaptive Sampling: Dynamische Stichprobengrößen basierend auf Systemressourcen (>16GB: Full Dataset, <16GB: Optimized Sampling)
    % - Begründung: Gewährleistung experimenteller Durchführbarkeit auf verschiedenen Hardware-Konfigurationen
    % - ohne Kompromittierung der statistischen Validität durch stratified random sampling preservation
    % - SCIENTIFIC_MODE=1 enforcement für >16GB RAM forciert Full-Dataset-Usage für maximale wissenschaftliche Rigorosität
    %
    % Stratifizierte Klassenbalancierung: Proportional erhaltene Attack/Normal-Verhältnisse bei reduzierten Stichproben
    % - Begründung: Preservation der ursprünglichen Datenverteilung verhindert Sampling-Bias
    % - und erhält realistische Klassen-Proportionen für Transfer-Evaluation ohne artificial class balancing
    % - Class Imbalance: 83% normal vs. 17% attack in CIC-IDS-2017 reflects realistic network environments

    Die empirische Evaluation basiert auf zwei etablierten Benchmark-Datensätzen, die komplementäre Perspektiven auf Netzwerk-Anomalieerkennung bieten und eine systematische Temporal-Transfer-Analyse über eine 19-Jahre-Technologie-Gap ermöglichen.

    \textbf{NSL-KDD (Network Security Laboratory – Knowledge Discovery and Data Mining, 2009)} stellt eine kuratierte Revision des ursprünglichen KDD Cup 99-Datensatzes dar, bei der systematische Duplikate und Trainingsbiases eliminiert wurden \parencite{Tavallaee2009}. Der Datensatz umfasst \textbf{125.973 Trainingssamples} und \textbf{22.544 dedizierte Testsamples}, die vier Hauptangriffskategorien abdecken: Denial-of-Service (DoS, 36\% der Attacks), Probe (11\%), Remote-to-Local (R2L, <1\%) und User-to-Root (U2R, <0,1\%). Die extreme Klassenimbalance bei U2R-Angriffen (nur 52 Samples im Testset) spiegelt realistische Szenarien wider, stellt jedoch eine methodische Herausforderung für ML-Algorithmen dar. Der Feature-Space besteht aus \textbf{41 connection-basierten Attributen}, darunter kategoriale Variablen (Protokoll-Typ: TCP/UDP/ICMP, Service-Typ, Flag-Status) und numerische Metriken (Verbindungsdauer, übertragene Bytes, Fehlerrate). Die Daten basieren auf simulierten Netzwerkangriffen aus dem Jahr 1998 (Lincoln Laboratory, MIT), was eine kontrollierte, aber potenziell limitierte externe Validität impliziert \parencite{McHugh2000}.

    \textbf{CIC-IDS-2017 (Canadian Institute for Cybersecurity Intrusion Detection System, 2017)} repräsentiert eine moderne Alternative mit \textbf{2.830.540 Samples} aus realistischen Netzwerkumgebungen (captured traffic, nicht simuliert). Die Datenerfassung erfolgte über fünf Tage (3.–7. Juli 2017) in einer realitätsnahen Testumgebung mit 25 Nutzern und 50 Maschinen \parencite{Sharafaldin2018}. Im Gegensatz zu NSL-KDD umfasst CIC-IDS-2017 \textbf{14 moderne Angriffskategorien}, darunter zeitgemäße Bedrohungen wie Heartbleed-Exploits, SQL-Injection, Cross-Site-Scripting (XSS), Botnet-Aktivitäten und DDoS-Varianten. Der Feature-Space ist mit \textbf{79 bidirektionalen Flow-Features} deutlich umfangreicher und beinhaltet erweiterte statistische Charakterisierungen wie Inter-Arrival-Time-Statistiken, Paket-Size-Distributionen und Flow-basierte Anomalie-Indikatoren. Die Klassenverteilung (83\% Normal, 17\% Attack) ist realistischer als bei NSL-KDD und vermeidet die dort beobachteten Extremimbalanzen.

    Die \textbf{Stichprobenauswahl} folgt einem \textbf{memory-adaptiven Sampling-Protokoll}, das die experimentelle Durchführbarkeit auf verschiedenen Hardware-Konfigurationen gewährleistet, ohne die statistische Validität zu kompromittieren. Bei Systemen mit \textbf{>16 GB RAM} wird der vollständige Datensatz verwendet (SCIENTIFIC\_MODE=1 enforcement), während bei geringeren Ressourcen eine \textbf{stratifizierte Zufallsstichprobe} gezogen wird, die die ursprüngliche Klassenverteilung proportional erhält. Für NSL-KDD wird aufgrund der moderaten Datensatzgröße (148k Samples) stets der komplette Datensatz verwendet. Für CIC-IDS-2017 erfolgt bei Memory-Constraints ein stratifiziertes Downsampling auf 200k–500k Samples (7–18\% des Originals), wobei alle Angriffskategorien proportional repräsentiert bleiben. Dieses Vorgehen verhindert Sampling-Bias und erhält die externe Validität für Transfer-Evaluationen.

    Die \textbf{Zugangslogistik} erfolgte über offizielle Repositories der Canadian Institute for Cybersecurity (University of New Brunswick), die beide Datensätze unter Open-Access-Lizenz bereitstellen. Ethische Bedenken sind nicht relevant, da die Daten vollständig anonymisiert und ohne personenbezogene Informationen vorliegen. Die Datensätze wurden im März 2025 heruntergeladen und mittels SHA-256-Checksummen auf Integrität validiert.

    \subsection{Experimenteller Ablauf und Evaluationsframework}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - EXPERIMENTELLER ABLAUF
    %
    % 8-stufige automatisierte Experimentalpipeline mit wissenschaftlicher Präzision:
    %
    % Phase 1 - Data Exploration (01_data_exploration.py):
    % - Begründung: Console-basierte EDA mit Verteilungsanalyse und Feature-Charakterisierung für Datenqualitäts-Assessment
    % - Funktionalität: Dataset Loading Verification, Class Distribution Analysis, Sample Data Inspection
    % - Output: Console-only validation, keine persistenten Files für explorative Analyse
    %
    % Phase 2 - Baseline Training (02_baseline_training.py):
    % - Begründung: 6 Baseline-Algorithmen (Random Forest, Logistic Regression, Decision Tree, Naive Bayes, k-NN, Linear SVM)
    % - etablieren interpretierbare Referenz-Performance für comparative evaluation mit advanced methods
    % - Class-Imbalance-Handling: class_weight="balanced" für algorithmus-agnostic fairness preservation
    %
    % Phase 3 - Advanced Training (03_advanced_training.py):
    % - Begründung: 6 Advanced-Algorithmen (XGBoost, LightGBM, Gradient Boosting, Extra Trees, MLP, Voting Classifier)
    % - repräsentieren state-of-the-art ML-Praxis für Cybersecurity-Anwendungen mit ensemble-based robustness
    % - Failed-Model-Handling: Graceful Degradation mit Dokumentation gescheiterter Trainings für transparency
    %
    % Phase 4 - Cross-Validation (04_cross_validation.py):
    % - Begründung: 5-Fold Stratified Cross-Validation für Within-Dataset-Robustheit mit statistischer Aggregation
    % - Statistical Testing: Paired t-tests, Cohen's d effect sizes, Bootstrap confidence intervals
    % - Class-Stratification: Erhaltung der Attack/Normal-Proportionen in allen CV-Folds
    %
    % Phase 5 - Cross-Dataset Evaluation (05_cross_dataset_evaluation.py):
    % - Begründung: Kernexperiment - Bidirektionale Transfer-Evaluation mit PCA-Alignment für fair comparison
    % - Transfer-Metriken: Transfer Ratio, Generalization Gap, Domain Divergence quantification
    % - Feature Alignment: 6 semantically harmonized features aus 41+79 original dimensions
    %
    % Phase 6 - Harmonized Evaluation (06_harmonized_evaluation.py):
    % - Begründung: Feature-harmonisierte Evaluation mit Incremental Learning für CIC-Full-Dataset (2.8M samples)
    % - SGDClassifier partial_fit() für memory-efficient large-scale processing
    % - Adaptive Sample Weighting: Dynamic class weight adjustment für realistic deployment scenarios
    %
    % Phase 7 - Results Summary (07_generate_results_summary.py):
    % - Begründung: Konsolidierte Ergebnisaggregation mit statistischen Tests für comprehensive reporting
    % - Output: CSV/JSON consolidation mit timing analysis integration und metadata preservation
    %
    % Phase 8 - Paper Figures (08_generate_paper_figures.py + 09/10_enhanced_visualizations.py):
    % - Begründung: Wissenschaftliche Publication-Ready Visualisierung mit 300 DPI, PDF-Export
    % - Scientific Standards: LaTeX-compatible formatting, colorblind-friendly palettes
    % - Enhanced Visualizations: Experimente 09/10 für zusätzliche scientific accurate visualizations und repository enhancement
    %
    % Technisches Setting: Python 3.8+, scikit-learn 1.3+, XGBoost 1.7+, LightGBM 3.3+
    % - Begründung: Etablierte ML-Ecosystem mit stabilen APIs und wissenschaftlicher Reproduzierbarkeit
    % - Platform-Independence: Cross-platform compatibility mit automated environment validation
    %
    % Fault-Tolerance: Incremental Result Storage mit automatischer Recovery bei Systemfehlern
    % - Begründung: Resiliente Experimentaldurchführung bei long-running computational experiments
    % - mit kritischen Systemressourcen und potential hardware failures
    % - Checkpoint-System: Periodische Model-State-Speicherung für Unterbrechungsresilienz

    \subsection{Feature-Engineering und Harmonisierung}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - FEATURE-ENGINEERING
    %
    % Zwei-Ebenen-Harmonisierungsstrategie mit wissenschaftlicher Präzision:
    %
    % Level 1 - Semantische Feature-Mappings:
    % - Begründung: NSL-KDD (41D) ↔ CIC-IDS-2017 (79D) Domain-Knowledge-basierte Korrespondenz
    % - erhält semantische Bedeutung und ermöglicht interpretierbare Cross-Dataset-Vergleiche ohne Informationsverlust
    % - Gemeinsame Features: duration, forward_bytes, backward_bytes, total_bytes, bytes_per_second, byte_ratio
    % - Feature-Space-Reduktion: 6 harmonisierte Features für Cross-Dataset-Kompatibilität bei preserved network semantics
    % - Validation: Correlation analysis zwischen original und harmonized features für semantic preservation
    %
    % Level 2 - Statistische Alignment-Pipeline:
    % - Begründung: StandardScaler-Normalisierung mit Z-Score-Transformation using Source-Dataset-Statistiken
    % - eliminiert scale-abhängige Bias und gewährleistet numerische Stabilität über verschiedene Algorithmus-Familien
    % - PCA-Projektion: 20 Hauptkomponenten für latente Repräsentation (Explained Variance >95%)
    % - optimaler Trade-off zwischen Dimensionsreduktion und Informationserhalt für robuste Transfer-Performance
    % - Domain Divergence Quantifizierung: Wasserstein-Distance zwischen aligned Feature-Distributionen
    % - theoretisch fundierte Messung der Dataset-Ähnlichkeit korreliert mit erwartbarer Transfer-Performance
    %
    % Feature-Alignment-Pipeline: Automatisierte Skalierung, Normalisierung und Validierung
    % - Begründung: Standardisierte Preprocessing-Pipeline eliminiert manual preprocessing bias
    % - und gewährleistet consistent feature engineering across experimental phases
    % - Robust Error Handling: UTF-8 parsing, missing value imputation, outlier detection
    %
    % Variance-Explained-Retention: Minimum 95% kumulativer Varianzerhalt bei PCA-Transformation
    % - Begründung: Optimaler Trade-off zwischen Dimensionsreduktion und Informationserhalt
    % - für robuste Transfer-Performance ohne overfitting zu source-domain characteristics
    %
    % Validierung: Comprehensive Unit Tests für Alignment-Konsistenz, Shape-Kompatibilität, numerische Stabilität
    % - Begründung: Automated quality assurance für complex feature transformation pipeline
    % - prevents silent failures in high-dimensional feature space alignment procedures

    \subsection{Modellauswahl und Hyperparameter-Konfiguration}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - MODELLAUSWAHL
    %
    % Baseline-Modelle (interpretierbare, effiziente Verfahren):
    % - Begründung: Etablierte Referenzalgorithmen mit unterschiedlichen Lernparadigmen
    % - (parametrisch/non-parametrisch, linear/non-linear) für comprehensive Baseline-Charakterisierung
    %
    % Random Forest: n_estimators=200, max_depth=25, class_weight="balanced"
    % - Begründung: Robust ensemble method mit inherent feature importance für interpretability
    % - Overfitting-Resistance durch bootstrap aggregation und controlled tree depth
    %
    % Logistic Regression: solver="saga", max_iter=2000, C=1.0, L2-Regularisierung
    % - Begründung: Linear baseline mit probabilistic output für calibrated confidence estimates
    % - SAGA solver für large-scale datasets mit L2 regularization für generalization
    %
    % Decision Tree: max_depth=25, min_samples_split=10, Overfitting-Prävention
    % - Begründung: Highly interpretable model mit explicit decision rules für cybersecurity domain experts
    % - Depth limitation prevents memorization von training-specific attack patterns
    %
    % k-Nearest Neighbors: k=3, algorithm="kd_tree", metric="euclidean"
    % - Begründung: Non-parametric method für local similarity-based classification
    % - KD-tree optimization für efficient nearest neighbor search in harmonized feature space
    %
    % Linear SVM: kernel="linear", C=1.0, probability=True für Soft-Voting
    % - Begründung: Maximum margin classifier mit theoretical generalization guarantees
    % - Linear kernel für computational efficiency und decision boundary interpretability
    %
    % Naive Bayes: GaussianNB ohne Hyperparameter-Tuning
    % - Begründung: Probabilistic baseline mit strong independence assumptions für comparison
    % - Parameter-free für fair comparison ohne hyperparameter optimization bias
    %
    % Advanced-Modelle (state-of-the-art Ensemble-Verfahren):
    % - Begründung: Repräsentieren moderne ML-Praxis für Cybersecurity-Anwendungen
    % - mit ensemble-based robustness und sophisticated feature interaction modeling
    %
    % XGBoost: n_estimators=400, max_depth=6, learning_rate=0.1, subsample=0.8
    % - Begründung: Gradient boosting mit advanced regularization für state-of-the-art performance
    % - tree_method="hist" für memory efficiency bei large-scale feature spaces
    %
    % LightGBM: n_estimators=400, learning_rate=0.05, num_leaves=31, class_weight="balanced"
    % - Begründung: Optimized gradient boosting mit faster training und lower memory usage
    % - Leaf-wise tree growth für better accuracy bei controlled overfitting risk
    %
    % MLP: hidden_layers=(128,64), activation="relu", solver="adam", early_stopping=True
    % - Begründung: Deep learning representative mit non-linear feature transformation capability
    % - Early stopping für automatic regularization und computational efficiency
    %
    % Voting Classifier: Soft-Voting-Kombination aller Basis-Lerner mit probabilistischen Gewichten
    % - Begründung: Meta-ensemble method für diversity-based robustness improvement
    % - Probabilistic averaging reduces individual model bias durch collective intelligence
    %
    % Hyperparameter-Strategie: Praxis-orientierte Default-Konfiguration ohne exhaustive Grid-Search
    % - Begründung: Fokus auf realistic deployment scenarios mit computational constraints
    % - verhindert hyperparameter overfitting zu specific benchmark characteristics
    %
    % Class Imbalance Handling: Adaptive Gewichtung durch class_weight="balanced"
    % - Begründung: Algorithmus-agnostic fairness für realistic class distributions (83% normal vs. 17% attack)
    % - SMOTE für NSL-KDD bei extremer minority class representation, Undersampling für CIC bei ausreichender sample size
    %
    % Failed-Model-Handling: Graceful Degradation mit Dokumentation gescheiterter Trainings
    % - Begründung: Vollständige Transparenz über Algorithmus-Limitationen bei memory-constrained
    % - oder high-dimensional Szenarien für realistic performance assessment

    \subsection{Evaluationsmetriken und Transfer-Learning-Assessment}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - EVALUATIONSMETRIKEN
    %
    % Within-Dataset-Performance-Metriken (internationale Standards):
    % - Begründung: Standard-Performancemetriken ermöglichen internationale Vergleichbarkeit
    % - und etablierte Benchmark-Referenzierung für Meta-Analysis-Integration
    %
    % Accuracy, Precision, Recall, F1-Score für binäre Klassifikation:
    % - Begründung: Comprehensive performance characterization mit Berücksichtigung von Precision/Recall-Trade-offs
    % - kritisch für cybersecurity applications bei asymmetrischen error costs (false positives vs. false negatives)
    %
    % ROC-AUC für probabilistische Bewertung und Threshold-Optimierung:
    % - Begründung: Threshold-agnostic performance measurement ermöglicht operational flexibility
    % - für deployment-specific operating point selection basierend auf business constraints
    %
    % 5-Fold Stratified Cross-Validation mit statistischer Signifikanz-Evaluation:
    % - Begründung: Robuste Within-Dataset-Validierung mit kontrollierten Zufallseffekten
    % - für stabile Performanceschätzungen und statistical power für significance testing
    %
    % Cross-Dataset-Transfer-Metriken (wissenschaftliche Neuheit und Innovation):
    % - Begründung: Erste systematische Quantifizierung der Cross-Domain-Generalisierungsfähigkeit
    % - mit theoretisch fundierten Metriken für praktische Transfer-Assessment
    %
    % Transfer Ratio: TR = Performance_target / Performance_source ∈ [0,1]
    % - Begründung: Quantifiziert relative Generalisierungsqualität, normalisiert für verschiedene Baseline-Performance-Level
    % - ermöglicht fair comparison zwischen models mit unterschiedlichen source-domain capabilities
    %
    % Generalization Gap: GG = Performance_source - Performance_target ≥ 0
    % - Begründung: Absolute Performancedifferenz zeigt praktische Transfer-Robustheit
    % - für Deployment-Entscheidungen und realistic performance expectation management
    %
    % Relative Performance Drop: RPD = (GG / Performance_source) × 100%
    % - Begründung: Prozentuale Degradation ermöglicht modellübergreifende Transfer-Qualitätsvergleiche
    % - standardized metric für ranking models by cross-domain robustness
    %
    % Domain Divergence: Wasserstein-Distanz zwischen Source- und Target-Feature-Distributionen
    % - Begründung: Theoretisch fundierte Messung der Dataset-Ähnlichkeit korreliert mit erwartbarer Transfer-Performance
    % - provides theoretical foundation für transfer learning expectations
    %
    % Statistische Validierung mit rigoroser Unsicherheitsquantifizierung:
    % - Begründung: Scientific rigor through comprehensive uncertainty assessment
    % - und statistical significance testing für robust scientific conclusions
    %
    % Bootstrap-Konfidenzintervalle für Unsicherheitsquantifizierung:
    % - Begründung: Non-parametrische Unsicherheitsschätzung ohne Verteilungsannahmen
    % - für robuste Inferenz bei complex performance distributions
    %
    % Paired t-Tests für Inter-Model-Vergleiche mit Bonferroni-Korrektur:
    % - Begründung: Kontrolle für Multiple Testing Problem bei systematischen Modellvergleichen
    % - statistische Signifikanz bei α=0.05 mit family-wise error rate control
    %
    % Cohen's d für praktische Signifikanz-Bewertung:
    % - Begründung: Quantifizierung praktischer Relevanz von Performance-Unterschieden
    % - für Decision-Making in produktiven IDS-Deployments über statistical significance hinaus
    %
    % Bidirektionale Transfer-Asymmetrie: |TR_forward - TR_reverse|
    % - Begründung: Identifikation asymmetrischer Generalisierungsmuster
    % - zwischen historical-to-modern vs. modern-to-historical Transfer-Richtungen für temporal robustness assessment

    \subsection{Experimentelle Kontrolle und Qualitätssicherung}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - EXPERIMENTELLE KONTROLLE
    %
    % Objektivität durch vollständige Automatisierung:
    % - Begründung: Vollständig automatisierte Pipeline ohne manuelle Hyperparameter-Intervention
    % - eliminiert subjektive Bias durch deterministische Ausführung und gewährleistet Inter-Operator-Reliabilität
    % - Deterministische Reproduzierbarkeit: RANDOM_STATE=42 für alle stochastischen Komponenten
    % - ermöglicht bit-level reproducibility für scientific integrity und peer review validation
    % - Standardisierte Train/Validation/Test-Splits respektieren offizielle Datensatz-Partitionierung
    % - prevents data leakage und maintains established benchmark comparability
    %
    % Reliabilität durch statistische Robustheit:
    % - Begründung: Robustheit durch 5-Fold Cross-Validation mit statistischer Aggregation
    % - und confidence interval estimation für uncertainty quantification
    % - Fault-Tolerant Execution: Incremental Result Storage mit automatischer Recovery
    % - resiliente Experimentaldurchführung bei long-running computational experiments
    % - Comprehensive Unit Testing: 15+ Tests für Feature-Alignment, Metrics, I/O-Pipeline
    % - automated quality assurance für complex experimental infrastructure
    %
    % Validität durch wissenschaftliche Standards:
    % - Begründung: Construct Validity durch Transfer-Metriken validiert durch Domain-Expert-Review
    % - und theoretical foundation in transfer learning literature
    % - External Validity: Etablierte Benchmark-Datensätze für wissenschaftliche Vergleichbarkeit
    % - mit internationaler research community und meta-analysis integration
    % - Internal Validity: Kontrollierte experimentelle Bedingungen, keine Data Leakage
    % - strict separation of training/validation/test sets across all experimental phases
    %
    % Computational Reproducibility durch umfassende Dokumentation:
    % - Begründung: Environment Validation durch validate_environment.py für System-Kompatibilität
    % - proactive identification von Platform-spezifischen Incompatibilities vor Experiment-Execution
    % - Dependency Management: requirements.txt mit Version-Pinning für deterministic software environment
    % - Git-Version-Control: Vollständige Code-Historie und Experiment-Tracking für scientific provenance
    %
    % Memory-Management und Computational Efficiency:
    % - Begründung: Adaptive Memory Management System für scalable execution
    % - Scientific Mode Enforcement: SCIENTIFIC_MODE=1 forciert Full-Dataset-Usage (>16GB RAM)
    % - Dynamic Memory Configuration basierend auf available system resources
    % - Real-Time Memory Monitoring mit MemoryMonitor Context Manager für Resource-Tracking
    % - prevents out-of-memory failures bei memory-intensive algorithmen

    \subsection{Memory-Adaptation und Computational Challenges}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - MEMORY-ADAPTATION
    %
    % Adaptive Memory Management System für scalable execution:
    % - Begründung: Scientific Mode Enforcement mit SCIENTIFIC_MODE=1 forciert Full-Dataset-Usage (>16GB RAM)
    % - für maximale wissenschaftliche Rigorosität und realistische large-scale evaluation scenarios
    % - Dynamic Memory Configuration: 32GB→30K Batch, 16GB→20K Batch, <16GB→Reduced Dataset
    % - adaptive scaling basierend auf available system resources ohne compromising statistical validity
    % - Real-Time Memory Monitoring: MemoryMonitor Context Manager für Resource-Tracking
    % - proactive resource management prevents Out-of-Memory-Failures bei memory-intensive Algorithmen
    % - Intelligent Garbage Collection: Explizite del statements und gc.collect() nach Processing-Phasen
    % - optimized memory footprint für sustained long-running computational experiments
    %
    % Incremental Learning für Large-Scale-Datasets (wissenschaftliche Innovation):
    % - Begründung: SGDClassifier mit partial_fit() für CIC-Full-Dataset (2.8M Samples)
    % - scalable Learning ohne Memory-Overflow, repräsentiert realistic deployment scenarios
    % - Chunk-basiertes Streaming: 20K Sample Batches mit Class-Balance-Preservation
    % - maintains statistical properties der original dataset distribution bei memory-efficient processing
    % - Adaptive Sample Weighting: Inverse Frequency Weighting basierend auf kumulativer Class-Distribution
    % - dynamic compensation für inherente class imbalance (83% normal vs. 17% attack) ohne aggressive resampling
    % - Online Class-Weight-Adaptation: Dynamic Gewichtsanpassung während Incremental Training
    % - real-time adaptation to evolving class distributions in streaming scenarios
    %
    % Computational Efficiency Optimizations für praxisnahe Performance:
    % - Begründung: Model-Specific Tuning mit tree_method="hist" für XGBoost, algorithm="kd_tree" für k-NN
    % - optimized algorithm variants für large-scale datasets ohne accuracy degradation
    % - Parallel Processing: n_jobs=-1 für CPU-Kern-Auslastung bei Ensemble-Modellen
    % - computational efficiency bei umfangreichen Experimenten ohne compromising result quality
    % - Early Stopping: MLP validation_fraction=0.1, max_iter=120 für Overfitting-Prävention
    % - automatic regularization und computational efficiency bei deep learning components
    % - Checkpoint System: Automatic Model Persistence mit pickle für Intermediate-Results
    % - fault-tolerance für continuation von unterbrochenen long-running experiments

    \subsection{Methodische Abgrenzungen und wissenschaftliche Limitationen}
    % PERFEKTIONIERTE ULTIMATIVE METHODIKSTRUKTUR - METHODISCHE ABGRENZUNGEN
    %
    % Bewusste Scope-Einschränkungen mit wissenschaftlicher Begründung:
    % - Begründung: Fokus auf Supervised Binary Classification (Normal vs. Attack) ohne Multi-Class-Granularität
    % - ermöglicht focused analysis der fundamental transfer learning challenges
    % - ohne complexity dilution durch attack-type-specific classification nuances
    % - Statische Feature-Sets ohne Real-Time-Feature-Engineering oder Online-Adaptation
    % - controlled experimental conditions für systematic comparative evaluation
    % - eliminiert dynamic feature engineering als confounding variable
    % - Cross-Sectional-Design ohne Temporal-Drift-Modeling oder Concept-Drift-Adaptation
    % - snapshot-based evaluation ermöglicht reproducible benchmarking
    % - temporal drift analysis würde separate longitudinal study design erfordern
    % - Ausschluss von Deep Learning: CNNs/RNNs aufgrund Computational-Constraints und Interpretability
    % - focus auf interpretable ML für cybersecurity domain expert acceptance
    % - computational feasibility für comprehensive comparative study
    %
    % Inhärente Limitationen mit transparent acknowledgment:
    % - Begründung: Dataset Temporal Gap mit 19-Jahre Technology-Evolution zwischen NSL-KDD (1998) und CIC-IDS-2017 (2017)
    % - reflects realistic challenge of temporal transfer in evolving cybersecurity landscape
    % - provides unique opportunity für temporal generalization assessment
    % - Feature Space Heterogenität: Nur 6 gemeinsame Features aus 41+79 verfügbaren Dimensionen
    % - conservative approach ensures semantic feature alignment validity
    % - trade-off zwischen feature richness und cross-dataset compatibility
    % - Simulated vs. Real Traffic: NSL-KDD basiert auf simulierten, CIC auf realen Network-Captures
    % - complementary data sources provide robustness assessment across simulation/reality gap
    % - Geographic Bias: Beide Datensätze stammen aus nordamerikanischen Forschungsumgebungen
    % - limitation für global generalizability, acknowledged für future international validation
    %
    % Threat to Validity mit wissenschaftlicher Reflexion:
    % - Begründung: External Validity - Generalisierbarkeit auf andere Netzwerk-Topologien unbekannt
    % - requires future validation in diverse network environments (IoT, 5G, cloud)
    % - Construct Validity - PCA-Alignment könnte task-relevante Information eliminieren
    % - trade-off zwischen dimensionality reduction und information preservation
    % - validated durch explained variance retention >95%
    % - Temporal Validity - Performance-Trends möglicherweise nicht auf zukünftige Bedrohungen übertragbar
    % - inherent limitation von static ML models für evolving threat landscape
    %
    % Ethische Überlegungen und Responsible AI:
    % - Begründung: Datenschutz durch Verwendung öffentlich verfügbarer, anonymisierter Benchmark-Datensätze
    % - Compliance mit Forschungsethik-Standards ohne Privacy-Risiken für reale Netzwerknutzer
    % - Computational Carbon Footprint: Energy-efficient Algorithm-Selection und Resource-Optimization
    % - Responsible AI-Practices durch minimierte environmental impact bei extensiven Experimenten
    % - Bias Awareness: Acknowledged geographic und temporal bias für transparent limitation disclosure


    \section{Ergebnisse}

    % Abbildung 1: Zentrale Performance-Übersicht
    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{../data/results/paper_figures/nsl_cic_model_performance_comparison.png}
        \caption{Vergleichende Modellperformance NSL-KDD vs. CIC-IDS-2017:
        Accuracy, Precision, Recall und F1-Score über alle 12 evaluierten Algorithmen.
        Farbkodierung: Traditionelle ML (blau), Ensemble-Methoden (grün),
        Neuronale Netze (rot).}
        \source{Eigene Darstellung.}
        \label{fig:performance_comparison}
    \end{figure}

    % Abbildung 2: Kernforschungsfrage - Cross-Dataset Transfer
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\textwidth]{../data/results/paper_figures/cross_dataset_transfer_analysis.png}
        \caption{Bidirektionale Cross-Dataset-Transfer-Analyse: Performance-Degradation
        beim Transfer NSL-KDD $\leftrightarrow$ CIC-IDS-2017. Balken zeigen
        Generalization Gap, Fehlerbalken indizieren Wasserstein Domain Divergence.}
        \source{Eigene Darstellung.}
        \label{fig:transfer_analysis}
    \end{figure}

    % Tabelle 1: Performance Summary (kompakt, Top-5)
    % Anpassen: Nur Top-5 Zeilen zeigen, Rest ins Anhang
    \input{../data/results/paper_figures/nsl_cic_performance_summary_table.tex}

    % Optional - wenn Platz: Dataset Comparison Overview
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.85\textwidth]{../data/results/paper_figures/dataset_comparison_overview.png}
        \caption{Dataset-spezifische Performance-Charakteristika:
        (a) Accuracy-Scatter NSL-KDD vs. CIC, (b) Metrik-Boxplots,
        (c) Statistische Signifikanztests (p < 0.05).}
        \source{Eigene Darstellung.}
        \label{fig:dataset_overview}
    \end{figure}

    \section{Diskussion}
    Ergebnisse interpretieren, Limitationen, Implikationen.

    \section{Fazit}
    Zentrale Punkte, Ausblick, Handlungsempfehlungen.

    % ---------- Literaturverzeichnis ----------
    \clearpage
    \printbibliography[title={Literaturverzeichnis}]

    % ---------- Anhangsverzeichnis (bei Bedarf) ----------
    \clearpage
    \section*{Anhangsverzeichnis}
    \addtoTOC{Anhangsverzeichnis}
    \begin{itemize}
        \item Anhang A: \hyperref[app:dataset_analysis]{Dataset-Charakterisierung und Explorative Analyse}
        \item Anhang B: \hyperref[app:within_dataset]{Within-Dataset Performance Details}
        \item Anhang C: \hyperref[app:cross_validation]{Cross-Validation und Statistische Analysen}
        \item Anhang D: \hyperref[app:transfer_analysis]{Cross-Dataset Transfer und Generalisierung}
        \item Anhang E: \hyperref[app:learning_curves]{Learning Curves und Trainingsanalysen}
        \item Anhang F: \hyperref[app:efficiency]{Computational Efficiency Analysis}
        \item Anhang G: \hyperref[app:dashboard]{Comprehensive Model Dashboard}
    \end{itemize}
    \clearpage

    % ---------- Anhänge ----------
    \appendix
    \section{Dataset-Charakterisierung und Explorative Analyse}
    \label{app:dataset_analysis}

    \subsection{NSL-KDD Attack Distribution}
    \label{app:nsl_attack_dist}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/paper_figures/nsl_attack_distribution_analysis.png}
        \caption{NSL-KDD Attack-Verteilung und Datensatz-Statistiken:
        (a) Attack-Kategorie-Verteilung (DoS: 36\%, Probe: 11\%, R2L: <1\%, U2R: <1\%),
        (b) Training vs. Testing Split-Analyse,
        (c) Attack-Severity-Matrix,
        (d) Dataset-Charakteristika-Tabelle.}
        \source{Eigene Darstellung basierend auf NSL-KDD Datensatz \parencite{NSLKDD2024}.}
        \label{fig:nsl_attack_dist}
    \end{figure}

    \paragraph{Interpretation der Attack-Verteilung}
    Die NSL-KDD-Verteilung zeigt eine Dominanz von DoS-Angriffen (36\% aller Attack-Samples), eine starke Klassenimbalance bei U2R (User-to-Root, <0.1\%) sowie gut repräsentierte Probe-Angriffe (11\%) für Pattern-Detection.

    \subsection{CIC-IDS-2017 Attack Distribution}
    \label{app:cic_attack_dist}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/paper_figures/cic_attack_distribution_analysis.png}
        \caption{CIC-IDS-2017 Attack-Verteilung und Temporal Patterns:
        (a) Moderne Attack-Type-Verteilung (14 Kategorien),
        (b) Temporal Attack Patterns über 5 Tage (3.-7. Juli 2017),
        (c) Attack-Severity-Heatmap,
        (d) Vergleichstabelle mit NSL-KDD.}
        \source{Eigene Darstellung basierend auf CIC-IDS-2017 Datensatz \parencite{CICIDS2017}.}
        \label{fig:cic_attack_dist}
    \end{figure}

    \paragraph{Unterschiede zu NSL-KDD}
    CIC-IDS-2017 zeichnet sich durch moderne Attack-Vektoren (Heartbleed, SQL-Injection, XSS), temporale Variabilität (Tag 3: DDoS-Peak, Tag 5: Port-Scan-Aktivität) und eine realistischere Klassenimbalance (83\% Normal, 17\% Attack) aus.

    \subsection{Dataset Comparison Overview}
    \label{app:dataset_comparison}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/paper_figures/dataset_comparison_overview.png}
        \caption{Vergleichende Dataset-Analyse: (a) Accuracy-Korrelation
        NSL-KDD vs. CIC (Pearson r = 0.72, p < 0.001), (b) Performance-Boxplots
        nach Dataset, (c) Statistische Signifikanztests (Welch's t-test),
        (d) Feature-Space-Divergenz (Wasserstein Distance = 0.148).}
        \source{Eigene Darstellung.}
        \label{fig:app_dataset_comparison}
    \end{figure}

    \section{Within-Dataset Performance Details}
    \label{app:within_dataset}

    \subsection{NSL-KDD ROC-Kurven}
    \label{app:nsl_roc}

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/roc_curves/nsl_kdd_baseline_scientific_roc.pdf}
            \caption{Baseline-Modelle (6 Algorithmen)}
            \label{fig:nsl_roc_baseline}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/roc_curves/nsl_kdd_advanced_scientific_roc.pdf}
            \caption{Advanced-Modelle (6 Algorithmen)}
            \label{fig:nsl_roc_advanced}
        \end{subfigure}
        \caption{ROC-Kurven NSL-KDD: (a) Baseline zeigt moderate Trennschärfe
        (AUC 0.35--1.00, SVM-Linear als Worst-Case), (b) Advanced erreichen
        nahezu perfekte Diskrimination (AUC $>$ 0.999 für XGBoost, LightGBM,
        Gradient Boosting). Diagonale = Random Classifier (AUC 0.5).}
        \source{Eigene Darstellung.}
        \label{fig:app_nsl_roc}
    \end{figure}

    \paragraph{ROC-Interpretation}
    Die ROC-Analyse zeigt bei \textbf{XGBoost/LightGBM} einen nahezu vertikalen Anstieg bei TPR $\approx$ 1.0 und FPR $\approx$ 0.0, was eine optimale Klassifikation indiziert. \textbf{SVM-Linear} erreicht eine AUC von 0.35 (schlechter als Random) aufgrund nicht-linearer Separierbarkeit, während \textbf{Naive Bayes} mit AUC = 0.95 eine gute probabilistische Kalibrierung trotz Feature-Unabhängigkeits-Annahme zeigt.

    \subsection{CIC-IDS-2017 ROC-Kurven}
    \label{app:cic_roc}

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/roc_curves/cic_ids_2017_baseline_scientific_roc.pdf}
            \caption{Baseline-Modelle}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/roc_curves/cic_ids_2017_advanced_scientific_roc.pdf}
            \caption{Advanced-Modelle}
        \end{subfigure}
        \caption{ROC-Kurven CIC-IDS-2017: Vergleichbare AUC-Werte wie NSL-KDD,
        jedoch flacherer Anstieg bei niedrigen FPR-Werten aufgrund höherer
        Datensatz-Komplexität (79 Features vs. 41, moderne Attack-Vektoren).}
        \source{Eigene Darstellung.}
        \label{fig:app_cic_roc}
    \end{figure}

    \subsection{Precision-Recall Kurven}
    \label{app:pr_curves}

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/precision_recall_curves/nsl_kdd_baseline_scientific_pr.pdf}
            \caption{NSL-KDD Baseline}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/precision_recall_curves/cic_ids_2017_baseline_scientific_pr.pdf}
            \caption{CIC-IDS-2017 Baseline}
        \end{subfigure}
        \\[1em]
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/precision_recall_curves/nsl_kdd_advanced_scientific_pr.pdf}
            \caption{NSL-KDD Advanced}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/precision_recall_curves/cic_ids_2017_advanced_scientific_pr.pdf}
            \caption{CIC-IDS-2017 Advanced}
        \end{subfigure}
        \caption{Precision-Recall Trade-Off-Analyse: PR-Kurven sind besonders
        informativ bei Klassenimbalance (CIC: 83\% Normal). Average Precision (AP)
        aggregiert Performance über alle Schwellenwerte. Baseline-Modelle zeigen
        stärkeren Precision-Drop bei hohem Recall (rechte Kurvenabschnitte) im
        Vergleich zu Advanced-Modellen.}
        \source{Eigene Darstellung.}
        \label{fig:app_pr_curves}
    \end{figure}

    \paragraph{PR-Kurven vs. ROC-Kurven}
    Bei starker Klassenimbalance (CIC-IDS-2017) können \textbf{ROC-Kurven} übermäßig optimistisch wirken, da hohe TN-Zahlen dominieren, während \textbf{PR-Kurven} sich auf die Minority Class (Attack) fokussieren und daher eine realistischere Einschätzung liefern. Ein Beispiel hierfür ist Random Forest CIC-IDS mit ROC-AUC = 1.0, aber AP = 0.999, was eine minimale Precision-Degradation bei hohem Recall zeigt.

    \subsection{Konfusionsmatrizen NSL-KDD}
    \label{app:cm_nsl}

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/confusion_matrices/nsl_kdd_baseline_scientific_cm.pdf}
            \caption{Baseline-Modelle}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/confusion_matrices/nsl_kdd_advanced_scientific_cm.pdf}
            \caption{Advanced-Modelle}
        \end{subfigure}
        \caption{Konfusionsmatrizen NSL-KDD (normalisiert pro True Label):
        Diagonalelemente = korrekte Klassifikationen (idealer Wert: 1.0).
        SVM-Linear zeigt starke False-Negative-Rate (dunklere Off-Diagonal-Werte).}
        \source{Eigene Darstellung.}
        \label{fig:app_cm_nsl}
    \end{figure}

    \subsection{Konfusionsmatrizen CIC-IDS-2017}
    \label{app:cm_cic}

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/confusion_matrices/cic_ids_2017_baseline_scientific_cm.pdf}
            \caption{Baseline-Modelle}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}
            \includegraphics[width=\textwidth]{../data/results/confusion_matrices/cic_ids_2017_advanced_scientific_cm.pdf}
            \caption{Advanced-Modelle}
        \end{subfigure}
        \caption{Konfusionsmatrizen CIC-IDS-2017: Naive Bayes zeigt charakteristische
        Bias zur Attack-Klasse (hohe False-Positive-Rate bei Normal $\rightarrow$
        Attack), während Decision Tree nahezu perfekte Klassifikation erreicht
        (Diagonale $\approx$ 1.0).}
        \source{Eigene Darstellung.}
        \label{fig:app_cm_cic}
    \end{figure}

    \section{Cross-Validation und Statistische Analysen}
    \label{app:cross_validation}

    \subsection{Cross-Validation Vergleich}
    \label{app:cv_comparison}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/paper_figures/cross_validation_comparison.png}
        \caption{Cross-Validation Performance-Vergleich NSL-KDD vs. CIC-IDS-2017:
        5-Fold stratifizierte CV mit Konfidenzintervallen (95\% CI).
        Fehlerbalken indizieren Variabilität über Folds.}
        \source{Eigene Darstellung.}
        \label{fig:app_cv_comparison}
    \end{figure}

    \subsection{CV Results Distribution}
    \label{app:cv_boxplot}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{../data/results/cv_results_boxplot.png}
        \caption{Boxplot-Verteilung der Cross-Validation Accuracy:
        Median (zentrale Linie), Interquartilbereich (Box), Whiskers (1.5×IQR),
        Ausreißer (Punkte). SVM-Linear zeigt extreme Variabilität über Folds
        (IQR = 0.43, Range = 0.33--0.83).}
        \source{Eigene Darstellung.}
        \label{fig:app_cv_boxplot}
    \end{figure}

    \paragraph{Variabilitäts-Interpretation}
    \begin{itemize}
        \item \textbf{Niedrige Variabilität (XGBoost, LightGBM):} IQR < 0.0005,
        indiziert robuste Performance unabhängig von Fold-Zusammensetzung
        \item \textbf{Hohe Variabilität (SVM-Linear):} IQR = 0.43,
        deutet auf Sensitivität gegenüber Datenpartitionierung hin
        \item \textbf{Ausreißer-Erkennung:} Naive Bayes zeigt 2 Ausreißer-Folds
        bei NSL-KDD (möglicherweise U2R-Attack-Cluster)
    \end{itemize}

    \subsection{Statistische Vergleichsanalysen}
    \label{app:statistical_comparison}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/confusion_matrices/cv_statistical_analysis_scientific.pdf}
        \caption{Statistische Vergleichsanalyse Top-5 Modelle:
        Pairwise t-Tests mit Bonferroni-Korrektur ($\alpha = 0.01$).
        Heatmap zeigt p-Werte, Sterne indizieren Signifikanz
        (*** p < 0.001, ** p < 0.01, * p < 0.05).}
        \source{Eigene Darstellung.}
        \label{fig:app_statistical}
    \end{figure}

    \paragraph{Signifikanz-Befunde}
    Aus statistical\_comparison.csv (gekürzt):
    \begin{itemize}
        \item \textbf{XGBoost vs. LightGBM:} Nicht signifikant (p = 0.385,
        Cohen's d = 0.31) $\rightarrow$ vergleichbare Performance
        \item \textbf{XGBoost vs. Naive Bayes:} Hochsignifikant (p < 0.001,
        Cohen's d = 26.76) $\rightarrow$ deutlicher Performance-Unterschied
        \item \textbf{Random Forest vs. Decision Tree:} Signifikant (p = 0.006,
        Cohen's d = 4.53) $\rightarrow$ RF überlegen
    \end{itemize}

    \subsection{Konvergenzanalyse}
    \label{app:convergence}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\textwidth]{../data/results/scientific_analysis/convergence_analysis/cv_convergence_analysis.pdf}
        \caption{Cross-Validation Konvergenzanalyse: Kumulative Mean Accuracy
        $\pm$ SD über Folds 1--5. Konvergenz ab Fold 3 indiziert ausreichende
        k-Wahl. Gestrichelte Linie = finale 5-Fold Mean.}
        \source{Eigene Darstellung.}
        \label{fig:app_convergence}
    \end{figure}

    \paragraph{Konvergenz-Interpretation}
    \begin{itemize}
        \item \textbf{Schnelle Konvergenz (Fold 2--3):} XGBoost, LightGBM,
        Random Forest $\rightarrow$ stabile Performance
        \item \textbf{Langsame Konvergenz (Fold 4--5):} SVM-Linear, Naive Bayes
        $\rightarrow$ höhere Sensitivität gegenüber Datensplit
        \item \textbf{Empfehlung:} k=5 ausreichend, k=10 würde SD nur marginal
        reduzieren (< 0.0001)
    \end{itemize}

    \section{Cross-Dataset Transfer und Generalisierung}
    \label{app:transfer_analysis}

    \subsection{Cross-Dataset Transfer Confusion Matrices}
    \label{app:transfer_cm}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/confusion_matrices/cross_dataset_scientific_transfer.pdf}
        \caption{Transfer-Learning Konfusionsmatrizen: (a) NSL-KDD $\rightarrow$
        CIC-IDS-2017, (b) CIC-IDS-2017 $\rightarrow$ NSL-KDD für XGBoost.
        Forward-Transfer (a) zeigt moderate Generalisierung (Target Acc = 0.827),
        Reverse-Transfer (b) zeigt starke Degradation (Target Acc = 0.431).}
        \source{Eigene Darstellung.}
        \label{fig:app_transfer_cm}
    \end{figure}

    \paragraph{Transfer-Pattern-Analyse}
    \begin{itemize}
        \item \textbf{Forward (NSL→CIC):} Off-Diagonal-Muster bei Normal→Attack
        (17\% FPR) aufgrund unterschiedlicher Feature-Skalierung
        \item \textbf{Reverse (CIC→NSL):} Starke Attack→Normal Misklassifikation
        (56\% FNR) durch veraltete Attack-Signaturen in NSL-KDD
        \item \textbf{Asymmetrie:} Forward-Transfer robuster aufgrund höherer
        NSL-KDD-Generalisierung (simplere Features)
    \end{itemize}

    \subsection{Harmonisierte Evaluation}
    \label{app:harmonized}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{../data/results/paper_figures/harmonized_evaluation_summary.png}
        \caption{Harmonisierte Cross-Dataset Evaluation: Performance bei
        PCA-alignierten Features (20 Komponenten, 94.7\% erklärte Varianz).
        Threshold-Tuning via Grid Search (0.1--0.9 in 0.1-Schritten).}
        \source{Eigene Darstellung.}
        \label{fig:app_harmonized}
    \end{figure}

    \paragraph{Harmonisierungs-Effekte}
    Vergleich native vs. harmonisierte Features:
    \begin{itemize}
        \item \textbf{NSL→CIC (native):} Target F1 = 0.0041 (XGBoost)
        \item \textbf{NSL→CIC (harmonisiert):} Target F1 = 0.5711 (139× Verbesserung)
        \item \textbf{Erklärung:} PCA-Alignment reduziert Feature-Distribution-Mismatch
        (Wasserstein Distance: 0.148 → 0.082)
    \end{itemize}

    \section{Learning Curves und Trainingsanalysen}
    \label{app:learning_curves}

    \subsection{Model Learning Curves}
    \label{app:learning_curves_detail}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/scientific_analysis/learning_curves/model_learning_curves.pdf}
        \caption{Lernkurven Top-3 Modelle bei variierenden Trainingsdatengrößen
        (1k--100k Samples): Training Accuracy (durchgezogene Linie) vs.
        Validation Accuracy (gestrichelt). Schattierte Bereiche = 95\% CI
        über 3 Wiederholungen.}
        \source{Eigene Darstellung.}
        \label{fig:app_learning_curves}
    \end{figure}

    \paragraph{Lernkurven-Interpretation}
    \begin{itemize}
        \item \textbf{XGBoost:}
        \begin{itemize}
            \item Konvergenz bei 20k Samples (Val Acc = 0.995)
            \item Minimaler Overfitting-Gap (Train-Val Diff < 0.005)
            \item Data-Efficient Learning (Plateau-Effekt)
        \end{itemize}
        \item \textbf{LightGBM:}
        \begin{itemize}
            \item Ähnliches Verhalten wie XGBoost
            \item Leicht höhere Varianz bei kleinen Sample Sizes (< 10k)
        \end{itemize}
        \item \textbf{Random Forest:}
        \begin{itemize}
            \item Langsame Konvergenz (Plateau erst bei 50k Samples)
            \item Höherer Overfitting-Gap (Train-Val Diff = 0.015 bei 10k)
            \item Indiziert Bedarf an größeren Trainingsdaten
        \end{itemize}
    \end{itemize}

    \paragraph{Praktische Implikationen}
    Für IDS-Deployments mit begrenzten Trainingsdaten:
    \begin{itemize}
        \item \textbf{< 10k Samples:} XGBoost/LightGBM bevorzugen
        (Val Acc > 0.98)
        \item \textbf{10k--50k Samples:} Alle Modelle vergleichbar
        \item \textbf{> 50k Samples:} Random Forest akzeptabel,
        aber längere Trainingszeit (siehe Anhang F)
    \end{itemize}

    \section{Computational Efficiency Analysis}
    \label{app:efficiency}

    \subsection{Timing Performance Analysis}
    \label{app:timing_analysis}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/scientific_analysis/comparative_analysis/timing_performance_analysis.pdf}
        \caption{Training Time vs. Accuracy Trade-Off: Bubble-Chart mit
        Bubble-Größe proportional zu Inferenzzeit. Optimale Modelle in oberer
        linker Region (hohe Accuracy, niedrige Training Time).}
        \source{Eigene Darstellung. Hardware: [aus README].}
        \label{fig:app_timing}
    \end{figure}

    \paragraph{Effizienz-Ranking}
    Aus timing\_analysis\_real\_timing\_summary.json:
    \begin{enumerate}
        \item \textbf{XGBoost:} Efficiency = 2.62 Acc/s (0.38s Training, 0.999 Acc)
        \item \textbf{LightGBM:} Efficiency = 1.38 Acc/s (0.58s Training, 0.814 Acc)
        \item \textbf{Decision Tree:} Efficiency = 0.46 Acc/s (2.17s, 0.997 Acc,
        Within-Dataset)
        \item \textbf{Random Forest (Forward):} Efficiency = 0.20 Acc/s (4.06s, 0.805 Acc)
        \item \textbf{Random Forest (Reverse):} Efficiency = 0.005 Acc/s
        (183.48s, 0.991 Acc, \textbf{48× langsamer als Forward!})
    \end{enumerate}

    \paragraph{Reverse-Transfer Performance-Paradox}
    CIC→NSL-KDD Training dauert signifikant länger trotz kleinerer Target-Größe:
    \begin{itemize}
        \item \textbf{Ursache:} Großer Source-Datensatz (CIC: 2.8M Samples)
        erfordert längeres Training
        \item \textbf{RF-spezifisch:} n\_estimators=200 × bootstrapping über
        2.8M Samples = 560M Samples total
        \item \textbf{Mitigation:} Sampling-basiertes Training (z.B. 100k
        Sample-Subset) reduziert Zeit auf \textasciitilde10s bei nur -2\% Accuracy
    \end{itemize}

    \subsection{Real-World Deployment Considerations}
    \label{app:deployment}

    \begin{table}[H]
        \centering
        \caption{Deployment-Szenarien und Modellempfehlungen}
        \label{tab:deployment}
        \begin{tabular}{lp{4cm}p{4cm}l}
            \toprule
            \textbf{Szenario} & \textbf{Constraints} & \textbf{Empfohlenes Modell}
            & \textbf{Grund} \\
            \midrule
            Real-Time IDS & < 100ms Inferenz & XGBoost & Schnellste Inferenz (23ms) \\
            Edge Device & < 1 MB Memory & Decision Tree & Kleinster Footprint \\
            High-Throughput & > 10k req/s & LightGBM & Beste Parallelisierung \\
            Transfer Learning & Cross-Domain & XGBoost & Robustester Transfer \\
            Incremental Learning & Online Updates & LightGBM & Native Online-Support \\
            \bottomrule
        \end{tabular}
        \source{Eigene Empfehlungen basierend auf experimentellen Ergebnissen.}
    \end{table}

    \section{Comprehensive Model Dashboard}
    \label{app:dashboard}

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../data/results/scientific_analysis/comparative_analysis/comprehensive_model_dashboard.pdf}
        \caption{Comprehensive Multi-Metrik Dashboard:
        (a) Radar-Chart aller Performance-Metriken,
        (b) Parallel-Koordinaten-Plot für Metrik-Interaktion,
        (c) Hierarchische Clustering-Dendrogram ähnlicher Modelle,
        (d) Principal Component Biplot für Modell-Distanzen im Metrik-Raum.}
        \source{Eigene Darstellung.}
        \label{fig:app_dashboard}
    \end{figure}

    \paragraph{Cluster-Analyse-Befunde}
    Hierarchisches Clustering (Ward-Linkage, Euclidean Distance, z-score normalisiert) identifiziert:
    \begin{itemize}
        \item \textbf{Cluster 1 (High-Performance):} XGBoost, LightGBM, Extra Trees
        (Distanz < 0.05)
        \item \textbf{Cluster 2 (Moderate):} Random Forest, Gradient Boosting,
        Decision Tree
        \item \textbf{Cluster 3 (Baseline):} Logistic Regression, k-NN, MLP
        \item \textbf{Outlier:} SVM-Linear (Distanz > 0.8 zu allen Clustern)
    \end{itemize}

    % ========= Ende =========
\end{document}
