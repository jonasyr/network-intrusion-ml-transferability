# ğŸ›¡ï¸ ML Network Anomaly Detection Research

## Eine experimentelle Analyse der EffektivitÃ¤t von Machine-Learning-Modellen fÃ¼r Anomalieerkennung im Netzwerkverkehr

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-1.2+-orange.svg)](https://scikit-learn.org)
[![Research](https://img.shields.io/badge/Research-In%20Progress-yellow.svg)](https://github.com/jonasyr/ml-network-anomaly-detection)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**Target**: 15-page scientific paper | **Timeline**: 10-12 weeks

---

## ğŸ“‹ Table of Contents

- [ğŸ¯ Project Overview](#-project-overview)
- [âœ¨ Features](#-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“Š Current Results](#-current-results)
- [ğŸ—ï¸ Project Structure](#ï¸-project-structure)
- [ğŸ”¬ Research Methodology](#-research-methodology)
- [ğŸ“ˆ Progress Tracking](#-progress-tracking)
- [ğŸ› ï¸ Development](#ï¸-development)
- [ğŸ“š References](#-references)

---

## ğŸ¯ Project Overview

This research project investigates the effectiveness of various machine learning models for network anomaly detection, focusing on intrusion detection systems (IDS). The study compares traditional ML approaches with modern techniques using established datasets NSL-KDD and CIC-IDS-2017.

### ğŸ“ Research Question

> **"Wie effektiv sind Machine-Learning-Modelle fÃ¼r Anomalieerkennung im Netzwerkverkehr? Eine experimentelle Analyse mit NSL-KDD und CIC-IDS-2017"**

### ğŸ¯ Objectives

- Compare effectiveness of different ML algorithms for network anomaly detection
- Analyze performance across different attack types and categories
- Evaluate cross-dataset generalization capabilities
- Provide comprehensive experimental analysis for academic publication

---

## âœ¨ Features

### âœ… **Currently Implemented**

#### ğŸ“Š **Data Analysis**

- âœ… NSL-KDD dataset integration
- âœ… Comprehensive data exploration
- âœ… Attack categorization (DoS, Probe, R2L, U2R)
- âœ… Statistical analysis pipeline
- âœ… Data quality validation

#### ğŸ¤– **Machine Learning**

- âœ… Baseline model implementation
  - Random Forest
  - Logistic Regression
  - Decision Tree
  - K-Nearest Neighbors
  - Naive Bayes
  - SVM (Linear)
- âœ… Preprocessing pipeline
- âœ… Class balancing (SMOTE, Undersampling)
- âœ… Model evaluation framework
- âœ… Advanced model suite (XGBoost, LightGBM, Gradient Boosting, Extra Trees, MLP, Soft Voting)

#### ğŸ”§ **Infrastructure**

- âœ… Modular code architecture
- âœ… Automated testing scripts
- âœ… Data validation pipeline
- âœ… Model persistence
- âœ… Results tracking

#### ğŸ“ˆ **Visualization**

- âœ… Attack distribution plots
- âœ… Feature analysis charts
- âœ… Model comparison graphics
- âœ… Interactive Jupyter notebooks

### ğŸ”„ **In Progress**

- ğŸ”„ CIC-IDS-2017 dataset integration
- ğŸ”„ Advanced model implementations
- ğŸ”„ Hyperparameter optimization
- ğŸ”„ Cross-dataset evaluation

### ğŸ“‹ **Planned**

- ğŸ“‹ Deep learning models (MLP, Autoencoders)
- ğŸ“‹ Ensemble methods
- ğŸ“‹ Feature selection optimization
- ğŸ“‹ Time-series analysis
- ğŸ“‹ Real-time detection simulation
- ğŸ“‹ Scientific paper publication

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.9+ required
python --version

# Git for version control
git --version
```

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/jonasyr/ml-network-anomaly-detection.git
cd ml-network-anomaly-detection

# 2. Set up Python environment
conda create -n anomaly-detection python=3.9
conda activate anomaly-detection

# 3. Install dependencies
pip install -r requirements.txt

# 4. Verify installation
python check_setup.py
```

### Dataset Setup

```bash
# Download NSL-KDD dataset
# Place files in data/raw/:
# - KDDTrain+.txt
# - KDDTest+.txt  
# - KDDTrain+_20Percent.txt

# Verify data
python check_data.py
```

### Quick Analysis

```bash
# Run baseline experiments
python scripts/run_baseline.py

# Run advanced experiments
python scripts/run_advanced.py

# Start Jupyter for detailed analysis
jupyter lab notebooks/01_data_exploration.ipynb

# Quick smoke test
python check_smoke.py
```

---

## ğŸ“Š Current Results

### Dataset Analysis (NSL-KDD 20% Subset)

| Metric | Value |
|--------|-------|
| **Total Records** | 25,192 |
| **Features** | 41 + 2 labels |
| **Attack Categories** | 5 (Normal, DoS, Probe, R2L, U2R) |
| **Attack Types** | 22 unique |
| **Data Quality** | âœ… Clean (no missing values) |

### Attack Distribution

```text
Normal Traffic: 53.4% (13,449 records)
DoS Attacks:    36.7% (9,234 records)  
Probe Attacks:   9.1% (2,289 records)
R2L Attacks:     0.8% (209 records)
U2R Attacks:     0.04% (11 records)
```

### Baseline Model Performance

| Model | Accuracy | F1-Score | Precision | Recall |
|-------|----------|----------|-----------|--------|
| **Random Forest** | 0.995 | 0.995 | 0.995 | 0.995 |
| **Decision Tree** | 0.993 | 0.993 | 0.993 | 0.993 |
| **Logistic Regression** | 0.945 | 0.944 | 0.946 | 0.945 |
| **K-Nearest Neighbors** | 0.967 | 0.967 | 0.968 | 0.967 |
| **Naive Bayes** | 0.821 | 0.815 | 0.835 | 0.821 |

> ğŸ“ *Results on balanced validation set (binary classification: Normal vs Attack)*

---

## ğŸ—ï¸ Project Structure

```text
ml-network-anomaly-detection/
â”œâ”€â”€ ğŸ“Š data/
â”‚   â”œâ”€â”€ raw/                    # Original datasets
â”‚   â”œâ”€â”€ processed/              # Cleaned & preprocessed data
â”‚   â”œâ”€â”€ models/                 # Trained model files
â”‚   â””â”€â”€ results/                # Analysis outputs
â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb      # âœ… Data analysis
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb         # ğŸ”„ Feature engineering
â”‚   â”œâ”€â”€ 03_baseline_models.ipynb       # ğŸ“‹ Model training
â”‚   â””â”€â”€ 04_evaluation.ipynb            # ğŸ“‹ Results analysis
â”œâ”€â”€ ğŸ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ preprocessor.py             # âœ… Data preprocessing
â”‚   â”‚   â””â”€â”€ loader.py                   # âœ… Dataset loading
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ baseline.py                 # âœ… Traditional ML models
â”‚   â”‚   â””â”€â”€ advanced.py                 # ğŸ“‹ Deep learning models
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ metrics.py                  # âœ… Evaluation framework
â”‚   â””â”€â”€ nsl_kdd_analyzer.py             # âœ… Main analysis engine
â”œâ”€â”€ ğŸ§ª experiments/                     # Experiment configurations
â”œâ”€â”€ ğŸ“ˆ reports/                         # Generated reports & papers
â”œâ”€â”€ ğŸ”§ scripts/
â”‚   â”œâ”€â”€ run_baseline.py                 # âœ… Quick model training
â”‚   â””â”€â”€ run_experiments.py              # ğŸ“‹ Full experiment suite
â”œâ”€â”€ check_*.py                          # âœ… Validation scripts
â”œâ”€â”€ requirements.txt                    # âœ… Dependencies
â””â”€â”€ README.md                           # ğŸ“– This file
```

**Legend:** âœ… Implemented | ğŸ”„ In Progress | ğŸ“‹ Planned

---

## ğŸ”¬ Research Methodology

### Phase 1: Foundation & Setup âœ…

- [x] Literature review (initial)
- [x] Environment setup
- [x] Dataset acquisition and validation
- [x] Basic analysis pipeline

### Phase 2: Data Preprocessing âœ…

- [x] NSL-KDD preprocessing pipeline
- [x] Feature analysis and selection
- [x] Class balancing strategies
- [x] Data validation framework

### Phase 3: Model Implementation ğŸ”„

- [x] Baseline traditional ML models
- [x] Evaluation framework
- [ ] Advanced ensemble methods
- [ ] Deep learning approaches
- [ ] Hyperparameter optimization

### Phase 4: Experimentation ğŸ“‹

- [ ] Cross-validation studies
- [ ] Cross-dataset evaluation
- [ ] Feature importance analysis
- [ ] Performance comparison

### Phase 5: Analysis & Documentation ğŸ“‹

- [ ] Statistical significance testing
- [ ] Results interpretation
- [ ] Scientific paper writing
- [ ] Peer review preparation

---

## ğŸ“ˆ Progress Tracking

### âœ… Week 1-2 (Current Status)

- Complete project infrastructure
- NSL-KDD dataset integrated and analyzed
- Baseline models trained and evaluated
- Initial results documented

### ğŸ”„ Week 3-4 (In Progress)

- CIC-IDS-2017 dataset integration
- Advanced model implementations
- Cross-dataset validation setup

### ğŸ“‹ Week 5-8 (Planned)

- Comprehensive model comparison
- Feature engineering optimization
- Statistical analysis and testing
- Performance benchmarking

### ğŸ“‹ Week 9-12 (Planned)

- Scientific paper writing
- Results validation and peer review
- Final documentation and submission

---

## ğŸ› ï¸ Development

### Running Tests

```bash
# Environment validation
python check_setup.py

# Data integrity check
python check_data.py

# Quick functionality test
python check_smoke.py

# Full test suite
python -m pytest tests/
```

### Adding New Models

```python
# Example: Adding a new classifier
from src.models.baseline import BaselineModels

baseline = BaselineModels()
baseline.add_model('my_model', MyClassifier())
baseline.train_all(X_train, y_train)
```

---

## ğŸ“š References

### Datasets

- **NSL-KDD**: [University of New Brunswick](https://www.unb.ca/cic/datasets/nsl.html)
- **CIC-IDS-2017**: [Canadian Institute for Cybersecurity](https://www.unb.ca/cic/datasets/ids-2017.html)

### Key Literature

- Tavallaee, M., et al. (2009). "A detailed analysis of the KDD CUP 99 data set"
- Sharafaldin, I., et al. (2018). "Toward generating a new intrusion detection dataset and intrusion traffic characterization"

### Technical Stack

- **Python 3.9+**: Core programming language
- **Scikit-Learn**: Machine learning framework
- **Pandas/NumPy**: Data manipulation
- **Matplotlib/Seaborn**: Visualization
- **Jupyter**: Interactive analysis

---

## ğŸ“ Academic Research Project

Targeting 15-page scientific paper submission

**Progress**: Week 2 of 12 | **Status**: On Track âœ…

---

**Last Updated**: June 28, 2025
