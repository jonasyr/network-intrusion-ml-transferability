# src/nsl_kdd_analyzer.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import warnings
import os
from pathlib import Path
warnings.filterwarnings('ignore')

# Set up plotting style
plt.style.use('default')
sns.set_palette("husl")

class NSLKDDAnalyzer:
    """
    Comprehensive analyzer for NSL-KDD dataset
    Designed for the intrusion detection research project
    """
    
    def __init__(self, data_dir="data/raw/nsl-kdd", output_dir="data/results"):
        """
        Initialize the analyzer with project directory structure
        
        Args:
            data_dir (str): Path to raw data directory
            output_dir (str): Path to results directory
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Define the 41 feature names based on NSL-KDD documentation
        self.feature_names = [
            # Basic features (1-9)
            'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 
            'dst_bytes', 'land', 'wrong_fragment', 'urgent',
            
            # Content features (10-22)
            'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 
            'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 
            'num_shells', 'num_access_files', 'num_outbound_cmds', 
            'is_hot_login', 'is_guest_login',
            
            # Time-based traffic features (23-31)
            'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 
            'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 
            'diff_srv_rate', 'srv_diff_host_rate',
            
            # Host-based traffic features (32-41)
            'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',
            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 
            'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 
            'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 
            'dst_host_srv_rerror_rate',
            
            # Labels
            'attack_type', 'difficulty_level'
        ]
        
        # Feature categories
        self.feature_categories = {
            'basic': list(range(0, 9)),
            'content': list(range(9, 22)),
            'time_based': list(range(22, 31)),
            'host_based': list(range(31, 41))
        }
        
        # Attack type to category mapping
        self.attack_categories = {
            'normal': 'Normal',
            # DoS attacks
            'back': 'DoS', 'land': 'DoS', 'neptune': 'DoS', 'pod': 'DoS', 
            'smurf': 'DoS', 'teardrop': 'DoS', 'mailbomb': 'DoS', 
            'apache2': 'DoS', 'processtable': 'DoS', 'udpstorm': 'DoS',
            # Probe attacks
            'ipsweep': 'Probe', 'nmap': 'Probe', 'portsweep': 'Probe', 
            'satan': 'Probe', 'mscan': 'Probe', 'saint': 'Probe',
            # R2L attacks
            'ftp_write': 'R2L', 'guess_passwd': 'R2L', 'imap': 'R2L', 
            'multihop': 'R2L', 'phf': 'R2L', 'spy': 'R2L', 
            'warezclient': 'R2L', 'warezmaster': 'R2L', 'sendmail': 'R2L', 
            'named': 'R2L', 'snmpgetattack': 'R2L', 'snmpguess': 'R2L', 
            'xlock': 'R2L', 'xsnoop': 'R2L', 'worm': 'R2L',
            # U2R attacks
            'buffer_overflow': 'U2R', 'loadmodule': 'U2R', 'perl': 'U2R', 
            'rootkit': 'U2R', 'httptunnel': 'U2R', 'ps': 'U2R', 
            'sqlattack': 'U2R'
        }
        
    def load_data(self, filename):
        """
        Load NSL-KDD data from file in the data/raw directory
        
        Args:
            filename (str): Name of the file (e.g., 'KDDTrain+.txt')
            
        Returns:
            pd.DataFrame: Loaded and processed data
        """
        file_path = self.data_dir / filename
        
        try:
            # Load data without headers (CSV format)
            data = pd.read_csv(file_path, names=self.feature_names, header=None)
            
            # Add attack category column
            data['attack_category'] = data['attack_type'].map(self.attack_categories)
            
            print(f"‚úì Successfully loaded data from {filename}")
            print(f"  Shape: {data.shape}")
            print(f"  Features: {len(self.feature_names)} (41 features + 2 labels)")
            
            return data
            
        except Exception as e:
            print(f"‚úó Error loading data from {filename}: {e}")
            return None
    
    def save_processed_data(self, data, filename_prefix):
        """
        Save processed data to the processed directory
        
        Args:
            data (pd.DataFrame): Processed data
            filename_prefix (str): Prefix for the saved file
        """
        processed_dir = Path("data/processed")
        processed_dir.mkdir(parents=True, exist_ok=True)
        
        # Save as CSV
        csv_path = processed_dir / f"{filename_prefix}_processed.csv"
        data.to_csv(csv_path, index=False)
        
        # Save as pickle for faster loading
        pkl_path = processed_dir / f"{filename_prefix}_processed.pkl"
        data.to_pickle(pkl_path)
        
        print(f"‚úì Saved processed data to:")
        print(f"  CSV: {csv_path}")
        print(f"  Pickle: {pkl_path}")
    
    def basic_info(self, data):
        """
        Display basic information about the dataset
        """
        print("\n" + "="*60)
        print("BASIC DATASET INFORMATION")
        print("="*60)
        
        print(f"Dataset Shape: {data.shape}")
        print(f"Total Records: {len(data):,}")
        print(f"Total Features: {data.shape[1]-2} (excluding labels)")
        print(f"Memory Usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # Check for missing values
        missing_values = data.isnull().sum()
        if missing_values.sum() > 0:
            print(f"\nMissing Values: {missing_values.sum()}")
            print(missing_values[missing_values > 0])
        else:
            print("\n‚úì No missing values found")
        
        # Data types
        print(f"\nData Types:")
        dtype_counts = data.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"  {dtype}: {count} columns")
            
    def analyze_features(self, data):
        """
        Analyze feature types and characteristics
        """
        print("\n" + "="*60)
        print("FEATURE ANALYSIS")
        print("="*60)
        
        numeric_features = []
        categorical_features = []
        
        # Separate features by type (excluding labels)
        for i, col in enumerate(data.columns[:-2]):  # Exclude attack_type and difficulty_level
            if data[col].dtype in ['int64', 'float64']:
                # Check if it's binary/categorical numeric
                unique_vals = data[col].nunique()
                if unique_vals <= 10:
                    categorical_features.append(col)
                else:
                    numeric_features.append(col)
            else:
                categorical_features.append(col)
        
        print(f"Numeric Features: {len(numeric_features)}")
        print(f"Categorical Features: {len(categorical_features)}")
        
        # Feature categories breakdown
        print(f"\nFeature Categories:")
        for category, indices in self.feature_categories.items():
            features = [self.feature_names[i] for i in indices]
            print(f"  {category.title()}: {len(features)} features")
            print(f"    {', '.join(features[:5])}{'...' if len(features) > 5 else ''}")
        
        # Analyze categorical features
        print(f"\nCategorical Feature Details:")
        categorical_data = data[categorical_features]
        for col in categorical_data.columns:
            unique_count = data[col].nunique()
            print(f"  {col}: {unique_count} unique values")
            if unique_count <= 10:
                print(f"    Values: {list(data[col].unique())}")
                
        return numeric_features, categorical_features
    
    def analyze_class_distribution(self, data):
        """
        Analyze attack types and categories distribution
        """
        print("\n" + "="*60)
        print("CLASS DISTRIBUTION ANALYSIS")
        print("="*60)
        
        # Attack type distribution
        attack_type_dist = data['attack_type'].value_counts()
        print(f"Attack Types Distribution:")
        print(f"Total unique attack types: {len(attack_type_dist)}")
        
        # Show top 10 most frequent
        print(f"\nTop 10 Most Frequent Attack Types:")
        for i, (attack, count) in enumerate(attack_type_dist.head(10).items(), 1):
            percentage = (count / len(data)) * 100
            print(f"  {i:2}. {attack:<15} {count:>8,} ({percentage:5.2f}%)")
        
        # Attack category distribution
        attack_cat_dist = data['attack_category'].value_counts()
        print(f"\nAttack Categories Distribution:")
        for category, count in attack_cat_dist.items():
            percentage = (count / len(data)) * 100
            print(f"  {category:<8} {count:>8,} ({percentage:5.2f}%)")
        
        # Difficulty level analysis
        if 'difficulty_level' in data.columns:
            diff_dist = data['difficulty_level'].value_counts().sort_index()
            print(f"\nDifficulty Level Distribution:")
            print(f"  Range: {data['difficulty_level'].min()} - {data['difficulty_level'].max()}")
            print(f"  Most common: Level {diff_dist.idxmax()} ({diff_dist.max():,} records)")
            
        return attack_type_dist, attack_cat_dist
    
    def statistical_summary(self, data, numeric_features):
        """
        Generate statistical summary for numeric features
        """
        print("\n" + "="*60)
        print("STATISTICAL SUMMARY")
        print("="*60)
        
        numeric_data = data[numeric_features]
        
        print("Numeric Features Statistics:")
        summary = numeric_data.describe()
        
        # Display summary in a more readable format
        for col in numeric_data.columns[:5]:  # Show first 5 features
            print(f"\n{col}:")
            print(f"  Mean: {summary.loc['mean', col]:.4f}")
            print(f"  Std:  {summary.loc['std', col]:.4f}")
            print(f"  Min:  {summary.loc['min', col]:.4f}")
            print(f"  Max:  {summary.loc['max', col]:.4f}")
            
        # Zero variance features
        zero_var_features = numeric_data.columns[numeric_data.var() == 0].tolist()
        if zero_var_features:
            print(f"\n‚ö†Ô∏è  Zero Variance Features: {len(zero_var_features)}")
            print(f"   {', '.join(zero_var_features)}")
        
        # High correlation features
        correlation_matrix = numeric_data.corr()
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_val = correlation_matrix.iloc[i, j]
                if abs(corr_val) > 0.9:
                    high_corr_pairs.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        corr_val
                    ))
        
        if high_corr_pairs:
            print(f"\n‚ö†Ô∏è  Highly Correlated Features (|r| > 0.9): {len(high_corr_pairs)} pairs")
            for feat1, feat2, corr in high_corr_pairs[:5]:
                print(f"   {feat1} - {feat2}: {corr:.3f}")
                
        return summary, high_corr_pairs
                
    def create_visualizations(self, data, attack_type_dist, attack_cat_dist, filename_prefix="analysis"):
        """
        Create comprehensive visualizations and save to results directory
        """
        print("\n" + "="*60)
        print("CREATING VISUALIZATIONS")
        print("="*60)
        
        # Set up the plotting
        plt.figure(figsize=(20, 15))
        
        # 1. Attack Category Distribution (Pie Chart)
        plt.subplot(2, 3, 1)
        attack_cat_dist.plot(kind='pie', autopct='%1.1f%%', startangle=90)
        plt.title('Attack Categories Distribution', fontsize=14, fontweight='bold')
        plt.ylabel('')
        
        # 2. Attack Category Distribution (Bar Chart)
        plt.subplot(2, 3, 2)
        attack_cat_dist.plot(kind='bar', color='skyblue', alpha=0.7)
        plt.title('Attack Categories Count', fontsize=14, fontweight='bold')
        plt.xlabel('Attack Category')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        
        # 3. Top 15 Attack Types
        plt.subplot(2, 3, 3)
        attack_type_dist.head(15).plot(kind='bar', color='lightcoral', alpha=0.7)
        plt.title('Top 15 Attack Types', fontsize=14, fontweight='bold')
        plt.xlabel('Attack Type')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        
        # 4. Difficulty Level Distribution
        plt.subplot(2, 3, 4)
        if 'difficulty_level' in data.columns:
            difficulty_dist = data['difficulty_level'].value_counts().sort_index()
            difficulty_dist.plot(kind='bar', color='lightgreen', alpha=0.7)
            plt.title('Difficulty Level Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Difficulty Level')
            plt.ylabel('Count')
        
        # 5. Protocol Type Distribution
        plt.subplot(2, 3, 5)
        if 'protocol_type' in data.columns:
            protocol_dist = data['protocol_type'].value_counts()
            protocol_dist.plot(kind='bar', color='orange', alpha=0.7)
            plt.title('Protocol Type Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Protocol Type')
            plt.ylabel('Count')
        
        # 6. Service Distribution (Top 10)
        plt.subplot(2, 3, 6)
        if 'service' in data.columns:
            service_dist = data['service'].value_counts().head(10)
            service_dist.plot(kind='bar', color='purple', alpha=0.7)
            plt.title('Top 10 Services', fontsize=14, fontweight='bold')
            plt.xlabel('Service')
            plt.ylabel('Count')
            plt.xticks(rotation=45)
        
        plt.tight_layout()
        
        # Save the plot
        plot_path = self.output_dir / f'{filename_prefix}_overview.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"‚úì Visualizations saved to: {plot_path}")
    
    def comprehensive_analysis(self, filename, save_processed=True):
        """
        Run complete analysis pipeline for a specific file
        
        Args:
            filename (str): Name of the NSL-KDD file to analyze
            save_processed (bool): Whether to save processed data
            
        Returns:
            pd.DataFrame: Processed data
        """
        print(f"üîç Starting Comprehensive Analysis for {filename}")
        print("="*60)
        
        # Load data
        data = self.load_data(filename)
        if data is None:
            return None
        
        # Basic information
        self.basic_info(data)
        
        # Feature analysis
        numeric_features, categorical_features = self.analyze_features(data)
        
        # Class distribution
        attack_type_dist, attack_cat_dist = self.analyze_class_distribution(data)
        
        # Statistical summary
        summary, high_corr_pairs = self.statistical_summary(data, numeric_features)
        
        # Create visualizations
        filename_prefix = filename.replace('.txt', '').replace('+', '_plus')
        self.create_visualizations(data, attack_type_dist, attack_cat_dist, filename_prefix)
        
        # Save processed data if requested
        if save_processed:
            self.save_processed_data(data, filename_prefix)
        
        print("\n" + "="*60)
        print("‚úÖ ANALYSIS COMPLETE")
        print("="*60)
        
        return data
    
    def compare_datasets(self, train_data, test_data):
        """
        Compare training and test datasets
        """
        print("\n" + "="*60)
        print("DATASET COMPARISON")
        print("="*60)
        
        print(f"Training Set:")
        print(f"  Records: {len(train_data):,}")
        print(f"  Attack types: {train_data['attack_type'].nunique()}")
        
        print(f"\nTest Set:")
        print(f"  Records: {len(test_data):,}")
        print(f"  Attack types: {test_data['attack_type'].nunique()}")
        
        # Find novel attacks in test set
        train_attacks = set(train_data['attack_type'].unique())
        test_attacks = set(test_data['attack_type'].unique())
        novel_attacks = test_attacks - train_attacks
        
        if novel_attacks:
            print(f"\n‚ö†Ô∏è  Novel attacks in test set (not in training): {len(novel_attacks)}")
            for attack in sorted(novel_attacks):
                count = (test_data['attack_type'] == attack).sum()
                print(f"   {attack}: {count} records")
        
        return novel_attacks

# Utility functions for the project
def setup_project_directories():
    """
    Ensure all necessary directories exist
    """
    directories = [
        "data/raw/nsl-kdd",
        "data/raw/cic-ids-2017",
        "data/processed",
        "data/models",
        "data/results",
        "docs/figures",
        "src/features",
        "src/preprocessing",
        "src/metrics",
        "src/visualization",
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        
    print("‚úì Project directories set up successfully")

if __name__ == "__main__":
    # Example usage
    setup_project_directories()
    
    analyzer = NSLKDDAnalyzer()
    
    print("üöÄ NSL-KDD Analyzer initialized!")
    print("Available files:")
    for file in analyzer.data_dir.glob("*.txt"):
        print(f"  - {file.name}")